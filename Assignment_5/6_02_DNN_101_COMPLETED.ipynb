{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmberMynott/AmberMynott-DataScience-GenAI-Submissions/blob/main/Assignment_5/6_02_DNN_101_COMPLETED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "665d1845-f81b-426e-96fb-9686c4ac82d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "b1995bc1-4204-4d19-b3a0-6219b97fcee9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "b203dc5c-169d-4209-ab79-6a5b42289f5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "e2a9fd2e-648d-46ad-8944-5304ca08bf37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 34702.3203\n",
            "Epoch [10/100], Loss: 27381.416\n",
            "Epoch [10/100], Loss: 32135.9395\n",
            "Epoch [10/100], Loss: 28475.0664\n",
            "Epoch [10/100], Loss: 27145.125\n",
            "Epoch [10/100], Loss: 22990.7871\n",
            "Epoch [10/100], Loss: 34069.5508\n",
            "Epoch [10/100], Loss: 51140.8906\n",
            "Epoch [20/100], Loss: 26486.2988\n",
            "Epoch [20/100], Loss: 37021.1953\n",
            "Epoch [20/100], Loss: 26186.2988\n",
            "Epoch [20/100], Loss: 27901.7188\n",
            "Epoch [20/100], Loss: 29693.7227\n",
            "Epoch [20/100], Loss: 34835.0195\n",
            "Epoch [20/100], Loss: 26120.9941\n",
            "Epoch [20/100], Loss: 15953.6826\n",
            "Epoch [30/100], Loss: 29584.8535\n",
            "Epoch [30/100], Loss: 32580.4863\n",
            "Epoch [30/100], Loss: 25158.5938\n",
            "Epoch [30/100], Loss: 30366.209\n",
            "Epoch [30/100], Loss: 36888.7188\n",
            "Epoch [30/100], Loss: 24940.4746\n",
            "Epoch [30/100], Loss: 26601.1328\n",
            "Epoch [30/100], Loss: 31364.0059\n",
            "Epoch [40/100], Loss: 25749.8516\n",
            "Epoch [40/100], Loss: 26061.2949\n",
            "Epoch [40/100], Loss: 33702.2891\n",
            "Epoch [40/100], Loss: 29322.3398\n",
            "Epoch [40/100], Loss: 29819.5352\n",
            "Epoch [40/100], Loss: 32677.2988\n",
            "Epoch [40/100], Loss: 26921.2734\n",
            "Epoch [40/100], Loss: 30419.3867\n",
            "Epoch [50/100], Loss: 29788.8477\n",
            "Epoch [50/100], Loss: 27997.6543\n",
            "Epoch [50/100], Loss: 30529.6445\n",
            "Epoch [50/100], Loss: 24289.7363\n",
            "Epoch [50/100], Loss: 27303.791\n",
            "Epoch [50/100], Loss: 28073.6836\n",
            "Epoch [50/100], Loss: 32882.3906\n",
            "Epoch [50/100], Loss: 39973.1211\n",
            "Epoch [60/100], Loss: 35742.3711\n",
            "Epoch [60/100], Loss: 26871.4141\n",
            "Epoch [60/100], Loss: 26944.5293\n",
            "Epoch [60/100], Loss: 30760.4141\n",
            "Epoch [60/100], Loss: 29046.2188\n",
            "Epoch [60/100], Loss: 23645.6074\n",
            "Epoch [60/100], Loss: 24787.9238\n",
            "Epoch [60/100], Loss: 26716.2715\n",
            "Epoch [70/100], Loss: 28110.4551\n",
            "Epoch [70/100], Loss: 24683.7051\n",
            "Epoch [70/100], Loss: 24126.9941\n",
            "Epoch [70/100], Loss: 35642.5898\n",
            "Epoch [70/100], Loss: 25344.5625\n",
            "Epoch [70/100], Loss: 23972.1641\n",
            "Epoch [70/100], Loss: 32184.3496\n",
            "Epoch [70/100], Loss: 4049.0933\n",
            "Epoch [80/100], Loss: 23557.625\n",
            "Epoch [80/100], Loss: 31818.1699\n",
            "Epoch [80/100], Loss: 27527.5449\n",
            "Epoch [80/100], Loss: 24636.8242\n",
            "Epoch [80/100], Loss: 22690.0742\n",
            "Epoch [80/100], Loss: 27377.1719\n",
            "Epoch [80/100], Loss: 28849.8887\n",
            "Epoch [80/100], Loss: 28299.9961\n",
            "Epoch [90/100], Loss: 30629.8652\n",
            "Epoch [90/100], Loss: 22054.8789\n",
            "Epoch [90/100], Loss: 27176.3574\n",
            "Epoch [90/100], Loss: 24242.6836\n",
            "Epoch [90/100], Loss: 20282.752\n",
            "Epoch [90/100], Loss: 26019.0996\n",
            "Epoch [90/100], Loss: 28114.0801\n",
            "Epoch [90/100], Loss: 35579.1875\n",
            "Epoch [100/100], Loss: 20616.3926\n",
            "Epoch [100/100], Loss: 22765.8223\n",
            "Epoch [100/100], Loss: 23560.875\n",
            "Epoch [100/100], Loss: 27313.9336\n",
            "Epoch [100/100], Loss: 24999.1621\n",
            "Epoch [100/100], Loss: 25293.9844\n",
            "Epoch [100/100], Loss: 25171.7461\n",
            "Epoch [100/100], Loss: 36023.7031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "eb4ce309-52e9-413f-e772-0ae21125a15e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 21168.029296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "25385a41-e46c-4f1e-8b0c-2b685a2dfdca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   19.434847   219.0\n",
              "1   18.456427    70.0\n",
              "2   19.085600   202.0\n",
              "3   23.223705   230.0\n",
              "4   18.548630   111.0\n",
              "..        ...     ...\n",
              "84  16.609005   153.0\n",
              "85  15.480692    98.0\n",
              "86  14.078079    37.0\n",
              "87  14.644762    63.0\n",
              "88  17.035822   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff3453e7-ff04-4e65-89b5-93cd7ab037a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19.434847</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18.456427</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.085600</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.223705</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18.548630</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>16.609005</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>15.480692</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>14.078079</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>14.644762</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>17.035822</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff3453e7-ff04-4e65-89b5-93cd7ab037a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff3453e7-ff04-4e65-89b5-93cd7ab037a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff3453e7-ff04-4e65-89b5-93cd7ab037a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-453dc9af-9e42-473b-98ea-f12141edacfc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-453dc9af-9e42-473b-98ea-f12141edacfc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-453dc9af-9e42-473b-98ea-f12141edacfc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_24cd6673-fb93-4227-8721-00d905f1a25b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_24cd6673-fb93-4227-8721-00d905f1a25b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          18.64315414428711,\n          16.547819137573242,\n          19.072998046875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I'm going to retrain the model but instead of having epochs = 100 like before, I'm going to have epochs = 1000"
      ],
      "metadata": {
        "id": "5Z8aNvGWK5nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmb53SujLA5o",
        "outputId": "a1152f9e-fbab-4e2c-dbcc-81e8cd355a50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6945.0659\n",
            "Epoch [10/1000], Loss: 7791.3438\n",
            "Epoch [10/1000], Loss: 5701.9312\n",
            "Epoch [10/1000], Loss: 7085.4507\n",
            "Epoch [10/1000], Loss: 6884.3848\n",
            "Epoch [10/1000], Loss: 7287.3286\n",
            "Epoch [10/1000], Loss: 6859.7075\n",
            "Epoch [10/1000], Loss: 3291.5115\n",
            "Epoch [20/1000], Loss: 7115.7163\n",
            "Epoch [20/1000], Loss: 5325.5547\n",
            "Epoch [20/1000], Loss: 6805.6167\n",
            "Epoch [20/1000], Loss: 6431.2847\n",
            "Epoch [20/1000], Loss: 7104.2637\n",
            "Epoch [20/1000], Loss: 3902.4119\n",
            "Epoch [20/1000], Loss: 5570.001\n",
            "Epoch [20/1000], Loss: 1104.7549\n",
            "Epoch [30/1000], Loss: 7301.665\n",
            "Epoch [30/1000], Loss: 3685.4348\n",
            "Epoch [30/1000], Loss: 5434.646\n",
            "Epoch [30/1000], Loss: 4899.7295\n",
            "Epoch [30/1000], Loss: 6334.002\n",
            "Epoch [30/1000], Loss: 4392.8955\n",
            "Epoch [30/1000], Loss: 5244.3213\n",
            "Epoch [30/1000], Loss: 6598.1279\n",
            "Epoch [40/1000], Loss: 4027.6719\n",
            "Epoch [40/1000], Loss: 5510.0859\n",
            "Epoch [40/1000], Loss: 5032.4785\n",
            "Epoch [40/1000], Loss: 3650.4297\n",
            "Epoch [40/1000], Loss: 6815.8311\n",
            "Epoch [40/1000], Loss: 4800.1377\n",
            "Epoch [40/1000], Loss: 4473.2554\n",
            "Epoch [40/1000], Loss: 1463.1096\n",
            "Epoch [50/1000], Loss: 5365.1299\n",
            "Epoch [50/1000], Loss: 3364.9744\n",
            "Epoch [50/1000], Loss: 4374.8374\n",
            "Epoch [50/1000], Loss: 5222.334\n",
            "Epoch [50/1000], Loss: 5221.4863\n",
            "Epoch [50/1000], Loss: 6122.0723\n",
            "Epoch [50/1000], Loss: 2615.0818\n",
            "Epoch [50/1000], Loss: 2318.6504\n",
            "Epoch [60/1000], Loss: 4757.4116\n",
            "Epoch [60/1000], Loss: 4661.2329\n",
            "Epoch [60/1000], Loss: 4906.9902\n",
            "Epoch [60/1000], Loss: 3642.8774\n",
            "Epoch [60/1000], Loss: 3722.1311\n",
            "Epoch [60/1000], Loss: 4902.915\n",
            "Epoch [60/1000], Loss: 4461.5308\n",
            "Epoch [60/1000], Loss: 2092.9753\n",
            "Epoch [70/1000], Loss: 4293.6938\n",
            "Epoch [70/1000], Loss: 4346.8682\n",
            "Epoch [70/1000], Loss: 3490.0051\n",
            "Epoch [70/1000], Loss: 5443.0942\n",
            "Epoch [70/1000], Loss: 4011.1819\n",
            "Epoch [70/1000], Loss: 5391.3623\n",
            "Epoch [70/1000], Loss: 2850.9805\n",
            "Epoch [70/1000], Loss: 10240.8867\n",
            "Epoch [80/1000], Loss: 3666.9075\n",
            "Epoch [80/1000], Loss: 4442.9712\n",
            "Epoch [80/1000], Loss: 3607.9424\n",
            "Epoch [80/1000], Loss: 4086.4539\n",
            "Epoch [80/1000], Loss: 3746.4937\n",
            "Epoch [80/1000], Loss: 4586.9663\n",
            "Epoch [80/1000], Loss: 5584.7764\n",
            "Epoch [80/1000], Loss: 3282.3872\n",
            "Epoch [90/1000], Loss: 4629.6582\n",
            "Epoch [90/1000], Loss: 4275.9565\n",
            "Epoch [90/1000], Loss: 3737.4419\n",
            "Epoch [90/1000], Loss: 4598.5063\n",
            "Epoch [90/1000], Loss: 4345.5249\n",
            "Epoch [90/1000], Loss: 4101.3853\n",
            "Epoch [90/1000], Loss: 3781.5874\n",
            "Epoch [90/1000], Loss: 1847.3096\n",
            "Epoch [100/1000], Loss: 4001.6189\n",
            "Epoch [100/1000], Loss: 4880.1304\n",
            "Epoch [100/1000], Loss: 3157.9299\n",
            "Epoch [100/1000], Loss: 3573.8645\n",
            "Epoch [100/1000], Loss: 4322.0156\n",
            "Epoch [100/1000], Loss: 4082.5073\n",
            "Epoch [100/1000], Loss: 4946.9741\n",
            "Epoch [100/1000], Loss: 5499.2305\n",
            "Epoch [110/1000], Loss: 3664.27\n",
            "Epoch [110/1000], Loss: 3865.7327\n",
            "Epoch [110/1000], Loss: 3031.2393\n",
            "Epoch [110/1000], Loss: 4361.7964\n",
            "Epoch [110/1000], Loss: 5412.5737\n",
            "Epoch [110/1000], Loss: 4078.6665\n",
            "Epoch [110/1000], Loss: 4406.0718\n",
            "Epoch [110/1000], Loss: 3261.7227\n",
            "Epoch [120/1000], Loss: 3562.8521\n",
            "Epoch [120/1000], Loss: 4619.1401\n",
            "Epoch [120/1000], Loss: 4012.978\n",
            "Epoch [120/1000], Loss: 4185.3794\n",
            "Epoch [120/1000], Loss: 3728.7439\n",
            "Epoch [120/1000], Loss: 4715.4595\n",
            "Epoch [120/1000], Loss: 3670.238\n",
            "Epoch [120/1000], Loss: 4949.2402\n",
            "Epoch [130/1000], Loss: 3743.7249\n",
            "Epoch [130/1000], Loss: 3938.3445\n",
            "Epoch [130/1000], Loss: 5397.5303\n",
            "Epoch [130/1000], Loss: 3085.7949\n",
            "Epoch [130/1000], Loss: 4441.4395\n",
            "Epoch [130/1000], Loss: 3047.604\n",
            "Epoch [130/1000], Loss: 4673.6738\n",
            "Epoch [130/1000], Loss: 4023.457\n",
            "Epoch [140/1000], Loss: 3897.8191\n",
            "Epoch [140/1000], Loss: 3132.052\n",
            "Epoch [140/1000], Loss: 3426.9705\n",
            "Epoch [140/1000], Loss: 4387.8672\n",
            "Epoch [140/1000], Loss: 4597.2388\n",
            "Epoch [140/1000], Loss: 4585.4224\n",
            "Epoch [140/1000], Loss: 4060.4844\n",
            "Epoch [140/1000], Loss: 4472.0498\n",
            "Epoch [150/1000], Loss: 4818.8291\n",
            "Epoch [150/1000], Loss: 3003.7417\n",
            "Epoch [150/1000], Loss: 2978.7725\n",
            "Epoch [150/1000], Loss: 3859.7803\n",
            "Epoch [150/1000], Loss: 5582.9844\n",
            "Epoch [150/1000], Loss: 4645.1675\n",
            "Epoch [150/1000], Loss: 3062.085\n",
            "Epoch [150/1000], Loss: 3335.5459\n",
            "Epoch [160/1000], Loss: 4503.8706\n",
            "Epoch [160/1000], Loss: 3400.1787\n",
            "Epoch [160/1000], Loss: 4154.562\n",
            "Epoch [160/1000], Loss: 4704.6807\n",
            "Epoch [160/1000], Loss: 3579.6963\n",
            "Epoch [160/1000], Loss: 3593.4055\n",
            "Epoch [160/1000], Loss: 3899.1743\n",
            "Epoch [160/1000], Loss: 1962.958\n",
            "Epoch [170/1000], Loss: 3758.6367\n",
            "Epoch [170/1000], Loss: 4126.0386\n",
            "Epoch [170/1000], Loss: 3162.5024\n",
            "Epoch [170/1000], Loss: 3809.4009\n",
            "Epoch [170/1000], Loss: 3833.3896\n",
            "Epoch [170/1000], Loss: 4519.9761\n",
            "Epoch [170/1000], Loss: 4355.0498\n",
            "Epoch [170/1000], Loss: 3185.8972\n",
            "Epoch [180/1000], Loss: 3438.833\n",
            "Epoch [180/1000], Loss: 3678.1306\n",
            "Epoch [180/1000], Loss: 4507.6392\n",
            "Epoch [180/1000], Loss: 3841.6802\n",
            "Epoch [180/1000], Loss: 4050.25\n",
            "Epoch [180/1000], Loss: 3132.052\n",
            "Epoch [180/1000], Loss: 4596.3882\n",
            "Epoch [180/1000], Loss: 5388.6411\n",
            "Epoch [190/1000], Loss: 3031.5312\n",
            "Epoch [190/1000], Loss: 3206.928\n",
            "Epoch [190/1000], Loss: 4245.542\n",
            "Epoch [190/1000], Loss: 3815.9634\n",
            "Epoch [190/1000], Loss: 3993.9111\n",
            "Epoch [190/1000], Loss: 4467.1816\n",
            "Epoch [190/1000], Loss: 4600.3872\n",
            "Epoch [190/1000], Loss: 294.2008\n",
            "Epoch [200/1000], Loss: 3557.0938\n",
            "Epoch [200/1000], Loss: 4356.2012\n",
            "Epoch [200/1000], Loss: 3919.7761\n",
            "Epoch [200/1000], Loss: 4037.1936\n",
            "Epoch [200/1000], Loss: 3915.0862\n",
            "Epoch [200/1000], Loss: 3162.4939\n",
            "Epoch [200/1000], Loss: 4052.6089\n",
            "Epoch [200/1000], Loss: 3484.5398\n",
            "Epoch [210/1000], Loss: 3927.7805\n",
            "Epoch [210/1000], Loss: 3451.6855\n",
            "Epoch [210/1000], Loss: 3923.6162\n",
            "Epoch [210/1000], Loss: 4118.2612\n",
            "Epoch [210/1000], Loss: 4269.6387\n",
            "Epoch [210/1000], Loss: 3417.8174\n",
            "Epoch [210/1000], Loss: 3766.7417\n",
            "Epoch [210/1000], Loss: 2633.8184\n",
            "Epoch [220/1000], Loss: 3139.4731\n",
            "Epoch [220/1000], Loss: 5040.3198\n",
            "Epoch [220/1000], Loss: 3130.2705\n",
            "Epoch [220/1000], Loss: 3260.1907\n",
            "Epoch [220/1000], Loss: 3835.7617\n",
            "Epoch [220/1000], Loss: 4055.9658\n",
            "Epoch [220/1000], Loss: 4389.6631\n",
            "Epoch [220/1000], Loss: 331.7602\n",
            "Epoch [230/1000], Loss: 3747.4905\n",
            "Epoch [230/1000], Loss: 2971.4275\n",
            "Epoch [230/1000], Loss: 4662.0537\n",
            "Epoch [230/1000], Loss: 3284.7773\n",
            "Epoch [230/1000], Loss: 3188.9055\n",
            "Epoch [230/1000], Loss: 3468.3755\n",
            "Epoch [230/1000], Loss: 5045.6929\n",
            "Epoch [230/1000], Loss: 5515.0635\n",
            "Epoch [240/1000], Loss: 4272.9556\n",
            "Epoch [240/1000], Loss: 3203.2412\n",
            "Epoch [240/1000], Loss: 4346.5605\n",
            "Epoch [240/1000], Loss: 4249.3774\n",
            "Epoch [240/1000], Loss: 3651.3269\n",
            "Epoch [240/1000], Loss: 3781.9661\n",
            "Epoch [240/1000], Loss: 2863.6267\n",
            "Epoch [240/1000], Loss: 2589.1995\n",
            "Epoch [250/1000], Loss: 4215.1006\n",
            "Epoch [250/1000], Loss: 3372.5549\n",
            "Epoch [250/1000], Loss: 3391.2666\n",
            "Epoch [250/1000], Loss: 4546.9917\n",
            "Epoch [250/1000], Loss: 3635.5168\n",
            "Epoch [250/1000], Loss: 4044.0117\n",
            "Epoch [250/1000], Loss: 3006.8174\n",
            "Epoch [250/1000], Loss: 2752.4304\n",
            "Epoch [260/1000], Loss: 3714.9856\n",
            "Epoch [260/1000], Loss: 3060.4207\n",
            "Epoch [260/1000], Loss: 3829.4055\n",
            "Epoch [260/1000], Loss: 3901.3386\n",
            "Epoch [260/1000], Loss: 4089.8252\n",
            "Epoch [260/1000], Loss: 3361.1399\n",
            "Epoch [260/1000], Loss: 4091.3105\n",
            "Epoch [260/1000], Loss: 2723.5581\n",
            "Epoch [270/1000], Loss: 4169.0435\n",
            "Epoch [270/1000], Loss: 3696.5925\n",
            "Epoch [270/1000], Loss: 3795.0786\n",
            "Epoch [270/1000], Loss: 3589.5791\n",
            "Epoch [270/1000], Loss: 4002.3394\n",
            "Epoch [270/1000], Loss: 3187.9563\n",
            "Epoch [270/1000], Loss: 3602.3538\n",
            "Epoch [270/1000], Loss: 114.8632\n",
            "Epoch [280/1000], Loss: 4023.8879\n",
            "Epoch [280/1000], Loss: 3991.8828\n",
            "Epoch [280/1000], Loss: 4308.0791\n",
            "Epoch [280/1000], Loss: 2974.9143\n",
            "Epoch [280/1000], Loss: 3031.9556\n",
            "Epoch [280/1000], Loss: 3684.8745\n",
            "Epoch [280/1000], Loss: 3322.9412\n",
            "Epoch [280/1000], Loss: 9388.9023\n",
            "Epoch [290/1000], Loss: 3678.7068\n",
            "Epoch [290/1000], Loss: 3665.5361\n",
            "Epoch [290/1000], Loss: 3214.959\n",
            "Epoch [290/1000], Loss: 3925.2673\n",
            "Epoch [290/1000], Loss: 3315.7449\n",
            "Epoch [290/1000], Loss: 4127.8662\n",
            "Epoch [290/1000], Loss: 3801.5085\n",
            "Epoch [290/1000], Loss: 529.5672\n",
            "Epoch [300/1000], Loss: 4445.8052\n",
            "Epoch [300/1000], Loss: 3356.8147\n",
            "Epoch [300/1000], Loss: 4028.5696\n",
            "Epoch [300/1000], Loss: 2694.7112\n",
            "Epoch [300/1000], Loss: 3285.2571\n",
            "Epoch [300/1000], Loss: 3694.5605\n",
            "Epoch [300/1000], Loss: 3632.5286\n",
            "Epoch [300/1000], Loss: 7976.3076\n",
            "Epoch [310/1000], Loss: 3938.2751\n",
            "Epoch [310/1000], Loss: 4028.1899\n",
            "Epoch [310/1000], Loss: 3857.0938\n",
            "Epoch [310/1000], Loss: 4064.4097\n",
            "Epoch [310/1000], Loss: 3266.9419\n",
            "Epoch [310/1000], Loss: 3227.8274\n",
            "Epoch [310/1000], Loss: 2818.1643\n",
            "Epoch [310/1000], Loss: 4625.1567\n",
            "Epoch [320/1000], Loss: 2931.8831\n",
            "Epoch [320/1000], Loss: 4113.6763\n",
            "Epoch [320/1000], Loss: 3431.6655\n",
            "Epoch [320/1000], Loss: 3532.3333\n",
            "Epoch [320/1000], Loss: 3662.5505\n",
            "Epoch [320/1000], Loss: 3434.3418\n",
            "Epoch [320/1000], Loss: 4012.4399\n",
            "Epoch [320/1000], Loss: 3582.1543\n",
            "Epoch [330/1000], Loss: 3561.613\n",
            "Epoch [330/1000], Loss: 3660.303\n",
            "Epoch [330/1000], Loss: 3585.9404\n",
            "Epoch [330/1000], Loss: 3840.865\n",
            "Epoch [330/1000], Loss: 3242.1824\n",
            "Epoch [330/1000], Loss: 3513.6924\n",
            "Epoch [330/1000], Loss: 3592.439\n",
            "Epoch [330/1000], Loss: 3247.0955\n",
            "Epoch [340/1000], Loss: 3650.9709\n",
            "Epoch [340/1000], Loss: 3250.1287\n",
            "Epoch [340/1000], Loss: 3834.7236\n",
            "Epoch [340/1000], Loss: 4034.9912\n",
            "Epoch [340/1000], Loss: 4148.8628\n",
            "Epoch [340/1000], Loss: 3223.7961\n",
            "Epoch [340/1000], Loss: 2726.8113\n",
            "Epoch [340/1000], Loss: 3038.033\n",
            "Epoch [350/1000], Loss: 3876.3743\n",
            "Epoch [350/1000], Loss: 3648.7549\n",
            "Epoch [350/1000], Loss: 3454.5896\n",
            "Epoch [350/1000], Loss: 2632.7483\n",
            "Epoch [350/1000], Loss: 2995.9231\n",
            "Epoch [350/1000], Loss: 4052.8074\n",
            "Epoch [350/1000], Loss: 3972.7361\n",
            "Epoch [350/1000], Loss: 4868.5293\n",
            "Epoch [360/1000], Loss: 3422.3213\n",
            "Epoch [360/1000], Loss: 3874.5596\n",
            "Epoch [360/1000], Loss: 3330.6685\n",
            "Epoch [360/1000], Loss: 3383.584\n",
            "Epoch [360/1000], Loss: 3309.4297\n",
            "Epoch [360/1000], Loss: 3444.2686\n",
            "Epoch [360/1000], Loss: 3621.7927\n",
            "Epoch [360/1000], Loss: 7081.1699\n",
            "Epoch [370/1000], Loss: 1819.7706\n",
            "Epoch [370/1000], Loss: 3744.6748\n",
            "Epoch [370/1000], Loss: 3877.4436\n",
            "Epoch [370/1000], Loss: 3553.9321\n",
            "Epoch [370/1000], Loss: 3614.2283\n",
            "Epoch [370/1000], Loss: 3993.6545\n",
            "Epoch [370/1000], Loss: 3966.1523\n",
            "Epoch [370/1000], Loss: 2114.9167\n",
            "Epoch [380/1000], Loss: 2934.7979\n",
            "Epoch [380/1000], Loss: 3764.071\n",
            "Epoch [380/1000], Loss: 3151.9614\n",
            "Epoch [380/1000], Loss: 3574.5962\n",
            "Epoch [380/1000], Loss: 3414.1016\n",
            "Epoch [380/1000], Loss: 4294.5532\n",
            "Epoch [380/1000], Loss: 3154.3152\n",
            "Epoch [380/1000], Loss: 4983.2236\n",
            "Epoch [390/1000], Loss: 3155.4836\n",
            "Epoch [390/1000], Loss: 2954.3855\n",
            "Epoch [390/1000], Loss: 3104.4016\n",
            "Epoch [390/1000], Loss: 3239.1702\n",
            "Epoch [390/1000], Loss: 3779.9468\n",
            "Epoch [390/1000], Loss: 3851.4709\n",
            "Epoch [390/1000], Loss: 4138.6719\n",
            "Epoch [390/1000], Loss: 4300.9834\n",
            "Epoch [400/1000], Loss: 3474.0356\n",
            "Epoch [400/1000], Loss: 2394.4949\n",
            "Epoch [400/1000], Loss: 3535.2979\n",
            "Epoch [400/1000], Loss: 2332.0273\n",
            "Epoch [400/1000], Loss: 3766.449\n",
            "Epoch [400/1000], Loss: 3864.6477\n",
            "Epoch [400/1000], Loss: 4876.999\n",
            "Epoch [400/1000], Loss: 2340.031\n",
            "Epoch [410/1000], Loss: 4507.6606\n",
            "Epoch [410/1000], Loss: 3386.4636\n",
            "Epoch [410/1000], Loss: 3220.3093\n",
            "Epoch [410/1000], Loss: 3306.0129\n",
            "Epoch [410/1000], Loss: 3156.9363\n",
            "Epoch [410/1000], Loss: 3156.0361\n",
            "Epoch [410/1000], Loss: 3371.8562\n",
            "Epoch [410/1000], Loss: 2538.1582\n",
            "Epoch [420/1000], Loss: 3762.3887\n",
            "Epoch [420/1000], Loss: 3199.0002\n",
            "Epoch [420/1000], Loss: 2902.8774\n",
            "Epoch [420/1000], Loss: 2448.6084\n",
            "Epoch [420/1000], Loss: 3187.4724\n",
            "Epoch [420/1000], Loss: 4006.5906\n",
            "Epoch [420/1000], Loss: 4407.4292\n",
            "Epoch [420/1000], Loss: 3838.7871\n",
            "Epoch [430/1000], Loss: 2793.7944\n",
            "Epoch [430/1000], Loss: 3763.4712\n",
            "Epoch [430/1000], Loss: 2918.5391\n",
            "Epoch [430/1000], Loss: 3676.325\n",
            "Epoch [430/1000], Loss: 3622.4043\n",
            "Epoch [430/1000], Loss: 2695.2661\n",
            "Epoch [430/1000], Loss: 4567.4219\n",
            "Epoch [430/1000], Loss: 54.5777\n",
            "Epoch [440/1000], Loss: 3272.2561\n",
            "Epoch [440/1000], Loss: 3254.802\n",
            "Epoch [440/1000], Loss: 3518.0037\n",
            "Epoch [440/1000], Loss: 2595.5481\n",
            "Epoch [440/1000], Loss: 3689.9949\n",
            "Epoch [440/1000], Loss: 4309.0376\n",
            "Epoch [440/1000], Loss: 3230.5906\n",
            "Epoch [440/1000], Loss: 1056.859\n",
            "Epoch [450/1000], Loss: 2459.8577\n",
            "Epoch [450/1000], Loss: 3478.4861\n",
            "Epoch [450/1000], Loss: 3269.5391\n",
            "Epoch [450/1000], Loss: 3029.3293\n",
            "Epoch [450/1000], Loss: 3711.5129\n",
            "Epoch [450/1000], Loss: 3481.1665\n",
            "Epoch [450/1000], Loss: 4304.2183\n",
            "Epoch [450/1000], Loss: 1756.3002\n",
            "Epoch [460/1000], Loss: 3564.7422\n",
            "Epoch [460/1000], Loss: 3423.9336\n",
            "Epoch [460/1000], Loss: 3633.2061\n",
            "Epoch [460/1000], Loss: 3008.6321\n",
            "Epoch [460/1000], Loss: 2845.4868\n",
            "Epoch [460/1000], Loss: 3882.9265\n",
            "Epoch [460/1000], Loss: 3185.373\n",
            "Epoch [460/1000], Loss: 3245.9741\n",
            "Epoch [470/1000], Loss: 3374.9553\n",
            "Epoch [470/1000], Loss: 3040.24\n",
            "Epoch [470/1000], Loss: 4383.4531\n",
            "Epoch [470/1000], Loss: 3534.6982\n",
            "Epoch [470/1000], Loss: 2670.6655\n",
            "Epoch [470/1000], Loss: 3803.2156\n",
            "Epoch [470/1000], Loss: 2490.8433\n",
            "Epoch [470/1000], Loss: 5983.2827\n",
            "Epoch [480/1000], Loss: 3282.3076\n",
            "Epoch [480/1000], Loss: 3755.6353\n",
            "Epoch [480/1000], Loss: 3343.9062\n",
            "Epoch [480/1000], Loss: 3095.1948\n",
            "Epoch [480/1000], Loss: 2540.9277\n",
            "Epoch [480/1000], Loss: 2995.5413\n",
            "Epoch [480/1000], Loss: 4271.7368\n",
            "Epoch [480/1000], Loss: 4461.48\n",
            "Epoch [490/1000], Loss: 2881.3374\n",
            "Epoch [490/1000], Loss: 3647.8662\n",
            "Epoch [490/1000], Loss: 4303.4688\n",
            "Epoch [490/1000], Loss: 4174.6304\n",
            "Epoch [490/1000], Loss: 2796.5698\n",
            "Epoch [490/1000], Loss: 2507.2827\n",
            "Epoch [490/1000], Loss: 2852.9255\n",
            "Epoch [490/1000], Loss: 4990.7554\n",
            "Epoch [500/1000], Loss: 4277.0674\n",
            "Epoch [500/1000], Loss: 3713.5469\n",
            "Epoch [500/1000], Loss: 3029.113\n",
            "Epoch [500/1000], Loss: 3918.1218\n",
            "Epoch [500/1000], Loss: 2744.3091\n",
            "Epoch [500/1000], Loss: 2953.4421\n",
            "Epoch [500/1000], Loss: 2492.6294\n",
            "Epoch [500/1000], Loss: 4279.7109\n",
            "Epoch [510/1000], Loss: 3791.1238\n",
            "Epoch [510/1000], Loss: 3100.2087\n",
            "Epoch [510/1000], Loss: 3811.3867\n",
            "Epoch [510/1000], Loss: 2641.592\n",
            "Epoch [510/1000], Loss: 3756.8206\n",
            "Epoch [510/1000], Loss: 3338.7043\n",
            "Epoch [510/1000], Loss: 2769.8193\n",
            "Epoch [510/1000], Loss: 1471.7507\n",
            "Epoch [520/1000], Loss: 3940.9617\n",
            "Epoch [520/1000], Loss: 2700.7124\n",
            "Epoch [520/1000], Loss: 3855.658\n",
            "Epoch [520/1000], Loss: 3075.5012\n",
            "Epoch [520/1000], Loss: 3772.1406\n",
            "Epoch [520/1000], Loss: 3072.5571\n",
            "Epoch [520/1000], Loss: 2602.459\n",
            "Epoch [520/1000], Loss: 3363.7988\n",
            "Epoch [530/1000], Loss: 2737.7158\n",
            "Epoch [530/1000], Loss: 3597.9648\n",
            "Epoch [530/1000], Loss: 2811.7136\n",
            "Epoch [530/1000], Loss: 4586.6455\n",
            "Epoch [530/1000], Loss: 3288.9575\n",
            "Epoch [530/1000], Loss: 2810.6431\n",
            "Epoch [530/1000], Loss: 3240.3994\n",
            "Epoch [530/1000], Loss: 1284.8899\n",
            "Epoch [540/1000], Loss: 2838.8562\n",
            "Epoch [540/1000], Loss: 3858.167\n",
            "Epoch [540/1000], Loss: 3104.3687\n",
            "Epoch [540/1000], Loss: 2366.8586\n",
            "Epoch [540/1000], Loss: 3893.3362\n",
            "Epoch [540/1000], Loss: 3891.5127\n",
            "Epoch [540/1000], Loss: 2788.8787\n",
            "Epoch [540/1000], Loss: 5368.9414\n",
            "Epoch [550/1000], Loss: 3088.2849\n",
            "Epoch [550/1000], Loss: 3458.4922\n",
            "Epoch [550/1000], Loss: 3770.7183\n",
            "Epoch [550/1000], Loss: 2369.6804\n",
            "Epoch [550/1000], Loss: 3802.8406\n",
            "Epoch [550/1000], Loss: 2886.3711\n",
            "Epoch [550/1000], Loss: 3357.5386\n",
            "Epoch [550/1000], Loss: 4370.5811\n",
            "Epoch [560/1000], Loss: 3243.5273\n",
            "Epoch [560/1000], Loss: 3576.2595\n",
            "Epoch [560/1000], Loss: 3265.9412\n",
            "Epoch [560/1000], Loss: 2752.5393\n",
            "Epoch [560/1000], Loss: 3312.968\n",
            "Epoch [560/1000], Loss: 3383.0056\n",
            "Epoch [560/1000], Loss: 3142.2256\n",
            "Epoch [560/1000], Loss: 4007.6108\n",
            "Epoch [570/1000], Loss: 3098.1978\n",
            "Epoch [570/1000], Loss: 2771.2188\n",
            "Epoch [570/1000], Loss: 3310.8447\n",
            "Epoch [570/1000], Loss: 3107.688\n",
            "Epoch [570/1000], Loss: 3594.04\n",
            "Epoch [570/1000], Loss: 2780.1602\n",
            "Epoch [570/1000], Loss: 3886.7634\n",
            "Epoch [570/1000], Loss: 5113.146\n",
            "Epoch [580/1000], Loss: 2756.8538\n",
            "Epoch [580/1000], Loss: 3519.9556\n",
            "Epoch [580/1000], Loss: 3624.6885\n",
            "Epoch [580/1000], Loss: 3444.5212\n",
            "Epoch [580/1000], Loss: 4415.3179\n",
            "Epoch [580/1000], Loss: 2729.7505\n",
            "Epoch [580/1000], Loss: 2194.3325\n",
            "Epoch [580/1000], Loss: 1761.7924\n",
            "Epoch [590/1000], Loss: 3254.8318\n",
            "Epoch [590/1000], Loss: 3012.103\n",
            "Epoch [590/1000], Loss: 3232.4331\n",
            "Epoch [590/1000], Loss: 3415.968\n",
            "Epoch [590/1000], Loss: 3421.0442\n",
            "Epoch [590/1000], Loss: 2990.6907\n",
            "Epoch [590/1000], Loss: 3129.1665\n",
            "Epoch [590/1000], Loss: 4294.3901\n",
            "Epoch [600/1000], Loss: 2491.7803\n",
            "Epoch [600/1000], Loss: 2809.0718\n",
            "Epoch [600/1000], Loss: 3445.0283\n",
            "Epoch [600/1000], Loss: 3723.5505\n",
            "Epoch [600/1000], Loss: 2970.6233\n",
            "Epoch [600/1000], Loss: 3567.8594\n",
            "Epoch [600/1000], Loss: 3368.6577\n",
            "Epoch [600/1000], Loss: 4398.0469\n",
            "Epoch [610/1000], Loss: 2885.5562\n",
            "Epoch [610/1000], Loss: 3979.3909\n",
            "Epoch [610/1000], Loss: 2526.3159\n",
            "Epoch [610/1000], Loss: 2963.322\n",
            "Epoch [610/1000], Loss: 3102.9902\n",
            "Epoch [610/1000], Loss: 3835.3184\n",
            "Epoch [610/1000], Loss: 3274.9512\n",
            "Epoch [610/1000], Loss: 141.4525\n",
            "Epoch [620/1000], Loss: 2756.9136\n",
            "Epoch [620/1000], Loss: 2754.2524\n",
            "Epoch [620/1000], Loss: 3777.1384\n",
            "Epoch [620/1000], Loss: 3469.9775\n",
            "Epoch [620/1000], Loss: 3645.8967\n",
            "Epoch [620/1000], Loss: 2753.1646\n",
            "Epoch [620/1000], Loss: 3293.9556\n",
            "Epoch [620/1000], Loss: 927.7565\n",
            "Epoch [630/1000], Loss: 3300.4375\n",
            "Epoch [630/1000], Loss: 3100.1787\n",
            "Epoch [630/1000], Loss: 3459.0369\n",
            "Epoch [630/1000], Loss: 3015.1558\n",
            "Epoch [630/1000], Loss: 3938.918\n",
            "Epoch [630/1000], Loss: 2866.7136\n",
            "Epoch [630/1000], Loss: 2711.1238\n",
            "Epoch [630/1000], Loss: 1019.3024\n",
            "Epoch [640/1000], Loss: 3071.5662\n",
            "Epoch [640/1000], Loss: 3835.615\n",
            "Epoch [640/1000], Loss: 3469.5139\n",
            "Epoch [640/1000], Loss: 2702.6143\n",
            "Epoch [640/1000], Loss: 3492.4988\n",
            "Epoch [640/1000], Loss: 2816.2158\n",
            "Epoch [640/1000], Loss: 2849.635\n",
            "Epoch [640/1000], Loss: 2642.449\n",
            "Epoch [650/1000], Loss: 3493.6912\n",
            "Epoch [650/1000], Loss: 3152.4675\n",
            "Epoch [650/1000], Loss: 3497.6877\n",
            "Epoch [650/1000], Loss: 2713.6956\n",
            "Epoch [650/1000], Loss: 2873.5579\n",
            "Epoch [650/1000], Loss: 3572.0159\n",
            "Epoch [650/1000], Loss: 2975.8694\n",
            "Epoch [650/1000], Loss: 975.5073\n",
            "Epoch [660/1000], Loss: 3364.585\n",
            "Epoch [660/1000], Loss: 2624.8538\n",
            "Epoch [660/1000], Loss: 4054.9265\n",
            "Epoch [660/1000], Loss: 3169.885\n",
            "Epoch [660/1000], Loss: 3782.4993\n",
            "Epoch [660/1000], Loss: 3486.8169\n",
            "Epoch [660/1000], Loss: 1566.964\n",
            "Epoch [660/1000], Loss: 3867.5405\n",
            "Epoch [670/1000], Loss: 2738.3162\n",
            "Epoch [670/1000], Loss: 3773.2893\n",
            "Epoch [670/1000], Loss: 3345.2031\n",
            "Epoch [670/1000], Loss: 3377.4968\n",
            "Epoch [670/1000], Loss: 3336.2534\n",
            "Epoch [670/1000], Loss: 2867.4744\n",
            "Epoch [670/1000], Loss: 2675.1226\n",
            "Epoch [670/1000], Loss: 1803.083\n",
            "Epoch [680/1000], Loss: 2580.6484\n",
            "Epoch [680/1000], Loss: 3851.2166\n",
            "Epoch [680/1000], Loss: 2843.1501\n",
            "Epoch [680/1000], Loss: 3606.3894\n",
            "Epoch [680/1000], Loss: 2871.0046\n",
            "Epoch [680/1000], Loss: 2830.803\n",
            "Epoch [680/1000], Loss: 3401.2119\n",
            "Epoch [680/1000], Loss: 3243.6143\n",
            "Epoch [690/1000], Loss: 2830.1177\n",
            "Epoch [690/1000], Loss: 3311.7761\n",
            "Epoch [690/1000], Loss: 2561.9045\n",
            "Epoch [690/1000], Loss: 3299.7231\n",
            "Epoch [690/1000], Loss: 2121.8589\n",
            "Epoch [690/1000], Loss: 3576.4319\n",
            "Epoch [690/1000], Loss: 3900.22\n",
            "Epoch [690/1000], Loss: 8868.6084\n",
            "Epoch [700/1000], Loss: 3276.0217\n",
            "Epoch [700/1000], Loss: 3782.0994\n",
            "Epoch [700/1000], Loss: 2542.5076\n",
            "Epoch [700/1000], Loss: 3433.4387\n",
            "Epoch [700/1000], Loss: 2758.4155\n",
            "Epoch [700/1000], Loss: 3294.937\n",
            "Epoch [700/1000], Loss: 2864.3643\n",
            "Epoch [700/1000], Loss: 1954.0281\n",
            "Epoch [710/1000], Loss: 3495.9502\n",
            "Epoch [710/1000], Loss: 2933.0278\n",
            "Epoch [710/1000], Loss: 3460.1487\n",
            "Epoch [710/1000], Loss: 3777.575\n",
            "Epoch [710/1000], Loss: 2924.437\n",
            "Epoch [710/1000], Loss: 2011.8478\n",
            "Epoch [710/1000], Loss: 3248.345\n",
            "Epoch [710/1000], Loss: 2605.7373\n",
            "Epoch [720/1000], Loss: 3147.3286\n",
            "Epoch [720/1000], Loss: 3443.4783\n",
            "Epoch [720/1000], Loss: 3427.6367\n",
            "Epoch [720/1000], Loss: 3052.4854\n",
            "Epoch [720/1000], Loss: 3316.7197\n",
            "Epoch [720/1000], Loss: 2517.3459\n",
            "Epoch [720/1000], Loss: 2899.0137\n",
            "Epoch [720/1000], Loss: 2525.9292\n",
            "Epoch [730/1000], Loss: 2932.9385\n",
            "Epoch [730/1000], Loss: 3136.1182\n",
            "Epoch [730/1000], Loss: 2812.3838\n",
            "Epoch [730/1000], Loss: 3555.2097\n",
            "Epoch [730/1000], Loss: 3651.2231\n",
            "Epoch [730/1000], Loss: 3061.9524\n",
            "Epoch [730/1000], Loss: 2641.6287\n",
            "Epoch [730/1000], Loss: 1872.179\n",
            "Epoch [740/1000], Loss: 2725.0081\n",
            "Epoch [740/1000], Loss: 2823.5491\n",
            "Epoch [740/1000], Loss: 3346.0408\n",
            "Epoch [740/1000], Loss: 2894.2532\n",
            "Epoch [740/1000], Loss: 3373.2705\n",
            "Epoch [740/1000], Loss: 3173.2219\n",
            "Epoch [740/1000], Loss: 3294.7188\n",
            "Epoch [740/1000], Loss: 3845.0195\n",
            "Epoch [750/1000], Loss: 3426.823\n",
            "Epoch [750/1000], Loss: 3046.9531\n",
            "Epoch [750/1000], Loss: 2969.9543\n",
            "Epoch [750/1000], Loss: 3129.5374\n",
            "Epoch [750/1000], Loss: 2790.1875\n",
            "Epoch [750/1000], Loss: 2564.5413\n",
            "Epoch [750/1000], Loss: 3253.6914\n",
            "Epoch [750/1000], Loss: 10662.0752\n",
            "Epoch [760/1000], Loss: 2870.134\n",
            "Epoch [760/1000], Loss: 3099.2756\n",
            "Epoch [760/1000], Loss: 3051.9612\n",
            "Epoch [760/1000], Loss: 3015.2461\n",
            "Epoch [760/1000], Loss: 4643.5381\n",
            "Epoch [760/1000], Loss: 2675.1897\n",
            "Epoch [760/1000], Loss: 2362.0986\n",
            "Epoch [760/1000], Loss: 1265.0671\n",
            "Epoch [770/1000], Loss: 3137.9087\n",
            "Epoch [770/1000], Loss: 3629.2986\n",
            "Epoch [770/1000], Loss: 2255.3591\n",
            "Epoch [770/1000], Loss: 3136.593\n",
            "Epoch [770/1000], Loss: 3559.0261\n",
            "Epoch [770/1000], Loss: 2808.4255\n",
            "Epoch [770/1000], Loss: 3010.5547\n",
            "Epoch [770/1000], Loss: 2937.0701\n",
            "Epoch [780/1000], Loss: 2478.0227\n",
            "Epoch [780/1000], Loss: 4038.4075\n",
            "Epoch [780/1000], Loss: 3123.7603\n",
            "Epoch [780/1000], Loss: 2947.9426\n",
            "Epoch [780/1000], Loss: 3768.47\n",
            "Epoch [780/1000], Loss: 2271.8955\n",
            "Epoch [780/1000], Loss: 2988.543\n",
            "Epoch [780/1000], Loss: 975.8779\n",
            "Epoch [790/1000], Loss: 2967.6953\n",
            "Epoch [790/1000], Loss: 2962.0239\n",
            "Epoch [790/1000], Loss: 3617.251\n",
            "Epoch [790/1000], Loss: 2871.7014\n",
            "Epoch [790/1000], Loss: 2793.5947\n",
            "Epoch [790/1000], Loss: 2782.033\n",
            "Epoch [790/1000], Loss: 3468.7817\n",
            "Epoch [790/1000], Loss: 2970.3579\n",
            "Epoch [800/1000], Loss: 2238.7427\n",
            "Epoch [800/1000], Loss: 3055.4749\n",
            "Epoch [800/1000], Loss: 3046.363\n",
            "Epoch [800/1000], Loss: 2947.457\n",
            "Epoch [800/1000], Loss: 2561.2886\n",
            "Epoch [800/1000], Loss: 3786.9849\n",
            "Epoch [800/1000], Loss: 3726.6487\n",
            "Epoch [800/1000], Loss: 4167.1689\n",
            "Epoch [810/1000], Loss: 2768.71\n",
            "Epoch [810/1000], Loss: 2671.5386\n",
            "Epoch [810/1000], Loss: 2629.228\n",
            "Epoch [810/1000], Loss: 3327.573\n",
            "Epoch [810/1000], Loss: 3196.8274\n",
            "Epoch [810/1000], Loss: 4255.1606\n",
            "Epoch [810/1000], Loss: 2535.7769\n",
            "Epoch [810/1000], Loss: 3197.3906\n",
            "Epoch [820/1000], Loss: 2850.5662\n",
            "Epoch [820/1000], Loss: 2239.3171\n",
            "Epoch [820/1000], Loss: 2703.8625\n",
            "Epoch [820/1000], Loss: 3333.7244\n",
            "Epoch [820/1000], Loss: 3788.9219\n",
            "Epoch [820/1000], Loss: 3437.1543\n",
            "Epoch [820/1000], Loss: 2988.2344\n",
            "Epoch [820/1000], Loss: 3101.658\n",
            "Epoch [830/1000], Loss: 3308.5381\n",
            "Epoch [830/1000], Loss: 2343.7358\n",
            "Epoch [830/1000], Loss: 3850.6665\n",
            "Epoch [830/1000], Loss: 2632.4749\n",
            "Epoch [830/1000], Loss: 3318.583\n",
            "Epoch [830/1000], Loss: 3046.0518\n",
            "Epoch [830/1000], Loss: 2679.5852\n",
            "Epoch [830/1000], Loss: 5488.0068\n",
            "Epoch [840/1000], Loss: 2681.3257\n",
            "Epoch [840/1000], Loss: 2578.0818\n",
            "Epoch [840/1000], Loss: 2515.6606\n",
            "Epoch [840/1000], Loss: 4078.312\n",
            "Epoch [840/1000], Loss: 4222.8394\n",
            "Epoch [840/1000], Loss: 2984.3113\n",
            "Epoch [840/1000], Loss: 2343.6326\n",
            "Epoch [840/1000], Loss: 925.5359\n",
            "Epoch [850/1000], Loss: 3429.239\n",
            "Epoch [850/1000], Loss: 2998.8555\n",
            "Epoch [850/1000], Loss: 2703.5249\n",
            "Epoch [850/1000], Loss: 3230.5569\n",
            "Epoch [850/1000], Loss: 3077.7356\n",
            "Epoch [850/1000], Loss: 3397.6494\n",
            "Epoch [850/1000], Loss: 2367.1802\n",
            "Epoch [850/1000], Loss: 3642.8687\n",
            "Epoch [860/1000], Loss: 3614.2817\n",
            "Epoch [860/1000], Loss: 2367.6316\n",
            "Epoch [860/1000], Loss: 2850.073\n",
            "Epoch [860/1000], Loss: 3786.5193\n",
            "Epoch [860/1000], Loss: 2514.969\n",
            "Epoch [860/1000], Loss: 3365.0828\n",
            "Epoch [860/1000], Loss: 2806.7822\n",
            "Epoch [860/1000], Loss: 1241.8862\n",
            "Epoch [870/1000], Loss: 2898.5962\n",
            "Epoch [870/1000], Loss: 3026.6411\n",
            "Epoch [870/1000], Loss: 4064.2837\n",
            "Epoch [870/1000], Loss: 2805.1423\n",
            "Epoch [870/1000], Loss: 3773.8247\n",
            "Epoch [870/1000], Loss: 2193.4819\n",
            "Epoch [870/1000], Loss: 2513.9248\n",
            "Epoch [870/1000], Loss: 1263.729\n",
            "Epoch [880/1000], Loss: 3911.6055\n",
            "Epoch [880/1000], Loss: 3372.9043\n",
            "Epoch [880/1000], Loss: 3068.5837\n",
            "Epoch [880/1000], Loss: 2875.0693\n",
            "Epoch [880/1000], Loss: 2586.4407\n",
            "Epoch [880/1000], Loss: 2631.25\n",
            "Epoch [880/1000], Loss: 2712.6721\n",
            "Epoch [880/1000], Loss: 2511.3596\n",
            "Epoch [890/1000], Loss: 2990.2068\n",
            "Epoch [890/1000], Loss: 1974.0822\n",
            "Epoch [890/1000], Loss: 2788.25\n",
            "Epoch [890/1000], Loss: 3125.0217\n",
            "Epoch [890/1000], Loss: 3066.9436\n",
            "Epoch [890/1000], Loss: 3126.5491\n",
            "Epoch [890/1000], Loss: 3929.5815\n",
            "Epoch [890/1000], Loss: 4965.2559\n",
            "Epoch [900/1000], Loss: 2701.7681\n",
            "Epoch [900/1000], Loss: 2919.9565\n",
            "Epoch [900/1000], Loss: 3284.4106\n",
            "Epoch [900/1000], Loss: 2725.3699\n",
            "Epoch [900/1000], Loss: 2919.9407\n",
            "Epoch [900/1000], Loss: 3156.1736\n",
            "Epoch [900/1000], Loss: 3331.126\n",
            "Epoch [900/1000], Loss: 3807.3472\n",
            "Epoch [910/1000], Loss: 3790.6692\n",
            "Epoch [910/1000], Loss: 3719.1692\n",
            "Epoch [910/1000], Loss: 2736.5081\n",
            "Epoch [910/1000], Loss: 2422.8503\n",
            "Epoch [910/1000], Loss: 2421.2983\n",
            "Epoch [910/1000], Loss: 2854.3638\n",
            "Epoch [910/1000], Loss: 3231.1985\n",
            "Epoch [910/1000], Loss: 989.8413\n",
            "Epoch [920/1000], Loss: 3163.4988\n",
            "Epoch [920/1000], Loss: 2535.7466\n",
            "Epoch [920/1000], Loss: 3919.1394\n",
            "Epoch [920/1000], Loss: 2486.9446\n",
            "Epoch [920/1000], Loss: 2902.8091\n",
            "Epoch [920/1000], Loss: 2783.864\n",
            "Epoch [920/1000], Loss: 2998.7581\n",
            "Epoch [920/1000], Loss: 7086.1714\n",
            "Epoch [930/1000], Loss: 2650.8508\n",
            "Epoch [930/1000], Loss: 2922.0044\n",
            "Epoch [930/1000], Loss: 2932.4968\n",
            "Epoch [930/1000], Loss: 3451.9827\n",
            "Epoch [930/1000], Loss: 3102.2021\n",
            "Epoch [930/1000], Loss: 2502.1147\n",
            "Epoch [930/1000], Loss: 3455.1958\n",
            "Epoch [930/1000], Loss: 2814.8291\n",
            "Epoch [940/1000], Loss: 3147.5513\n",
            "Epoch [940/1000], Loss: 3696.8499\n",
            "Epoch [940/1000], Loss: 2765.0054\n",
            "Epoch [940/1000], Loss: 2195.5916\n",
            "Epoch [940/1000], Loss: 3134.4111\n",
            "Epoch [940/1000], Loss: 3425.2583\n",
            "Epoch [940/1000], Loss: 2449.0562\n",
            "Epoch [940/1000], Loss: 5744.7905\n",
            "Epoch [950/1000], Loss: 2757.7783\n",
            "Epoch [950/1000], Loss: 2298.2539\n",
            "Epoch [950/1000], Loss: 2911.1296\n",
            "Epoch [950/1000], Loss: 3706.2605\n",
            "Epoch [950/1000], Loss: 2895.3459\n",
            "Epoch [950/1000], Loss: 3574.9023\n",
            "Epoch [950/1000], Loss: 2828.4832\n",
            "Epoch [950/1000], Loss: 2710.8032\n",
            "Epoch [960/1000], Loss: 3755.8364\n",
            "Epoch [960/1000], Loss: 2971.2705\n",
            "Epoch [960/1000], Loss: 2475.8335\n",
            "Epoch [960/1000], Loss: 3492.625\n",
            "Epoch [960/1000], Loss: 2583.5657\n",
            "Epoch [960/1000], Loss: 2458.3384\n",
            "Epoch [960/1000], Loss: 3332.4871\n",
            "Epoch [960/1000], Loss: 761.8976\n",
            "Epoch [970/1000], Loss: 2473.3884\n",
            "Epoch [970/1000], Loss: 3012.7412\n",
            "Epoch [970/1000], Loss: 2784.0256\n",
            "Epoch [970/1000], Loss: 3756.2942\n",
            "Epoch [970/1000], Loss: 2962.9675\n",
            "Epoch [970/1000], Loss: 3849.8467\n",
            "Epoch [970/1000], Loss: 2134.6555\n",
            "Epoch [970/1000], Loss: 2108.7542\n",
            "Epoch [980/1000], Loss: 3056.0955\n",
            "Epoch [980/1000], Loss: 4437.5737\n",
            "Epoch [980/1000], Loss: 2810.554\n",
            "Epoch [980/1000], Loss: 2600.1323\n",
            "Epoch [980/1000], Loss: 2266.6118\n",
            "Epoch [980/1000], Loss: 2845.6318\n",
            "Epoch [980/1000], Loss: 2995.6055\n",
            "Epoch [980/1000], Loss: 1111.0488\n",
            "Epoch [990/1000], Loss: 3104.4592\n",
            "Epoch [990/1000], Loss: 2540.6182\n",
            "Epoch [990/1000], Loss: 2882.0996\n",
            "Epoch [990/1000], Loss: 2849.6763\n",
            "Epoch [990/1000], Loss: 2614.3647\n",
            "Epoch [990/1000], Loss: 4360.9365\n",
            "Epoch [990/1000], Loss: 2284.6763\n",
            "Epoch [990/1000], Loss: 7105.5718\n",
            "Epoch [1000/1000], Loss: 2767.7688\n",
            "Epoch [1000/1000], Loss: 3178.0999\n",
            "Epoch [1000/1000], Loss: 2466.2344\n",
            "Epoch [1000/1000], Loss: 2574.561\n",
            "Epoch [1000/1000], Loss: 3718.6997\n",
            "Epoch [1000/1000], Loss: 2757.7144\n",
            "Epoch [1000/1000], Loss: 3547.7061\n",
            "Epoch [1000/1000], Loss: 699.0329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the loss still bounces around a bit, it's now reaching lower numbers, with the final loss value being 699 compared to 36023 before. This implies that increases the number of epochs has improved our model. In order to further test this, I'm going to evaluate the model like before."
      ],
      "metadata": {
        "id": "wYNZd3wuLTuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHhmZJHILxar",
        "outputId": "12bee31f-98db-43b7-94fe-04cb946d230e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2877.7706298828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our MSE has also significantly decreased; in fact, it's decreased by over 80% from 21168 to 2878. This furthers my idea that the model had improved.\n",
        "\n",
        "Finally, I am going to repeat what we did earlier by collecting the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions"
      ],
      "metadata": {
        "id": "Hc2vONFyL3dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5xkzLL-rMV4u",
        "outputId": "e772b150-9220-4a76-ad25-12bb768550b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   148.347992   219.0\n",
              "1   173.067734    70.0\n",
              "2   143.428619   202.0\n",
              "3   290.197235   230.0\n",
              "4   130.204163   111.0\n",
              "..         ...     ...\n",
              "84  116.300079   153.0\n",
              "85   88.164093    98.0\n",
              "86   74.084595    37.0\n",
              "87   66.077423    63.0\n",
              "88  148.458344   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-355821f1-a6a8-44fc-aed6-53a584be40fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148.347992</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>173.067734</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>143.428619</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>290.197235</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>130.204163</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>116.300079</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>88.164093</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>74.084595</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>66.077423</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>148.458344</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-355821f1-a6a8-44fc-aed6-53a584be40fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-355821f1-a6a8-44fc-aed6-53a584be40fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-355821f1-a6a8-44fc-aed6-53a584be40fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fb8febc6-b851-47b5-b41c-d64312262f5c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb8febc6-b851-47b5-b41c-d64312262f5c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fb8febc6-b851-47b5-b41c-d64312262f5c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_51a2a69a-2e15-43e2-9237-eb0426ad8e73\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_51a2a69a-2e15-43e2-9237-eb0426ad8e73 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          174.67796325683594,\n          111.5138168334961,\n          174.78746032714844\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While there is still error in these predictions, they are much more accurate than the model with only 100 epochs."
      ],
      "metadata": {
        "id": "FX9OKbYyMclF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "v31E9ODjK3-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuing with 1000 epochs from the exercise above, I'm now going to experiment by increasing the number of neurons."
      ],
      "metadata": {
        "id": "tq0QFbDeMqwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model with increased neurons\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(10, 10), # Increased from 5 neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10), # Increased from 5 neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 1) # Output layer remains 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "print(\"DiabetesModel class updated with 10 neurons in hidden layers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWQ-kDGRMy8z",
        "outputId": "f051f511-4e43-4825-ebf6-a09d3e666c38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiabetesModel class updated with 10 neurons in hidden layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now once again predicting and evaluating the model to see if this has made an improvement."
      ],
      "metadata": {
        "id": "bjdM1TbXNSCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I need to retrain the model"
      ],
      "metadata": {
        "id": "TfiK5Ns4NgmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer with the updated architecture\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop with 1000 epochs\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for inputs, targets in train_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg5b3LRwNirz",
        "outputId": "619cbd6d-e394-4e21-8c4c-9b5867815370"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 33904.2148\n",
            "Epoch [10/1000], Loss: 22684.0293\n",
            "Epoch [10/1000], Loss: 30570.6914\n",
            "Epoch [10/1000], Loss: 34103.1641\n",
            "Epoch [10/1000], Loss: 29310.0703\n",
            "Epoch [10/1000], Loss: 24896.3867\n",
            "Epoch [10/1000], Loss: 33524.4688\n",
            "Epoch [10/1000], Loss: 9720.6787\n",
            "Epoch [20/1000], Loss: 31786.9844\n",
            "Epoch [20/1000], Loss: 33524.3008\n",
            "Epoch [20/1000], Loss: 28248.3887\n",
            "Epoch [20/1000], Loss: 21912.3789\n",
            "Epoch [20/1000], Loss: 30960.6836\n",
            "Epoch [20/1000], Loss: 32924.0156\n",
            "Epoch [20/1000], Loss: 27962.9902\n",
            "Epoch [20/1000], Loss: 6245.9116\n",
            "Epoch [30/1000], Loss: 25008.5898\n",
            "Epoch [30/1000], Loss: 30736.3516\n",
            "Epoch [30/1000], Loss: 24757.4844\n",
            "Epoch [30/1000], Loss: 28279.709\n",
            "Epoch [30/1000], Loss: 29293.1914\n",
            "Epoch [30/1000], Loss: 32153.6797\n",
            "Epoch [30/1000], Loss: 30231.7461\n",
            "Epoch [30/1000], Loss: 51778.043\n",
            "Epoch [40/1000], Loss: 29835.0098\n",
            "Epoch [40/1000], Loss: 19475.0312\n",
            "Epoch [40/1000], Loss: 28935.6035\n",
            "Epoch [40/1000], Loss: 30665.2598\n",
            "Epoch [40/1000], Loss: 24075.6562\n",
            "Epoch [40/1000], Loss: 33469.1641\n",
            "Epoch [40/1000], Loss: 25882.7949\n",
            "Epoch [40/1000], Loss: 63338.8359\n",
            "Epoch [50/1000], Loss: 22872.1641\n",
            "Epoch [50/1000], Loss: 29784.3242\n",
            "Epoch [50/1000], Loss: 29806.9434\n",
            "Epoch [50/1000], Loss: 24657.0137\n",
            "Epoch [50/1000], Loss: 20393.6465\n",
            "Epoch [50/1000], Loss: 28742.4941\n",
            "Epoch [50/1000], Loss: 27054.7227\n",
            "Epoch [50/1000], Loss: 27613.1621\n",
            "Epoch [60/1000], Loss: 22923.8887\n",
            "Epoch [60/1000], Loss: 24259.6738\n",
            "Epoch [60/1000], Loss: 23838.502\n",
            "Epoch [60/1000], Loss: 19775.0176\n",
            "Epoch [60/1000], Loss: 26683.1172\n",
            "Epoch [60/1000], Loss: 21842.9902\n",
            "Epoch [60/1000], Loss: 27693.8262\n",
            "Epoch [60/1000], Loss: 34264.1758\n",
            "Epoch [70/1000], Loss: 21494.7793\n",
            "Epoch [70/1000], Loss: 16673.373\n",
            "Epoch [70/1000], Loss: 19802.2422\n",
            "Epoch [70/1000], Loss: 26577.2012\n",
            "Epoch [70/1000], Loss: 18302.459\n",
            "Epoch [70/1000], Loss: 25846.4902\n",
            "Epoch [70/1000], Loss: 19354.7715\n",
            "Epoch [70/1000], Loss: 1767.084\n",
            "Epoch [80/1000], Loss: 16423.168\n",
            "Epoch [80/1000], Loss: 22689.1289\n",
            "Epoch [80/1000], Loss: 16805.4688\n",
            "Epoch [80/1000], Loss: 22005.6348\n",
            "Epoch [80/1000], Loss: 11837.8447\n",
            "Epoch [80/1000], Loss: 20640.0566\n",
            "Epoch [80/1000], Loss: 13852.8447\n",
            "Epoch [80/1000], Loss: 10135.0605\n",
            "Epoch [90/1000], Loss: 14942.751\n",
            "Epoch [90/1000], Loss: 11443.7422\n",
            "Epoch [90/1000], Loss: 17131.5977\n",
            "Epoch [90/1000], Loss: 15414.1025\n",
            "Epoch [90/1000], Loss: 14530.0449\n",
            "Epoch [90/1000], Loss: 15375.54\n",
            "Epoch [90/1000], Loss: 10734.6494\n",
            "Epoch [90/1000], Loss: 19141.125\n",
            "Epoch [100/1000], Loss: 13681.627\n",
            "Epoch [100/1000], Loss: 11727.2793\n",
            "Epoch [100/1000], Loss: 11054.5127\n",
            "Epoch [100/1000], Loss: 9339.1436\n",
            "Epoch [100/1000], Loss: 8810.7246\n",
            "Epoch [100/1000], Loss: 10205.1533\n",
            "Epoch [100/1000], Loss: 12512.8193\n",
            "Epoch [100/1000], Loss: 3071.8523\n",
            "Epoch [110/1000], Loss: 9118.2959\n",
            "Epoch [110/1000], Loss: 7752.1763\n",
            "Epoch [110/1000], Loss: 8268.3301\n",
            "Epoch [110/1000], Loss: 8789.96\n",
            "Epoch [110/1000], Loss: 9523.3457\n",
            "Epoch [110/1000], Loss: 7028.5034\n",
            "Epoch [110/1000], Loss: 8287.374\n",
            "Epoch [110/1000], Loss: 4517.0649\n",
            "Epoch [120/1000], Loss: 6733.063\n",
            "Epoch [120/1000], Loss: 8001.4604\n",
            "Epoch [120/1000], Loss: 6396.7065\n",
            "Epoch [120/1000], Loss: 9081.3838\n",
            "Epoch [120/1000], Loss: 5867.1885\n",
            "Epoch [120/1000], Loss: 4572.4521\n",
            "Epoch [120/1000], Loss: 3964.2949\n",
            "Epoch [120/1000], Loss: 10913.8184\n",
            "Epoch [130/1000], Loss: 6255.4873\n",
            "Epoch [130/1000], Loss: 5423.6284\n",
            "Epoch [130/1000], Loss: 4825.8271\n",
            "Epoch [130/1000], Loss: 3463.3765\n",
            "Epoch [130/1000], Loss: 5577.1597\n",
            "Epoch [130/1000], Loss: 5564.1074\n",
            "Epoch [130/1000], Loss: 5095.3486\n",
            "Epoch [130/1000], Loss: 13291.3184\n",
            "Epoch [140/1000], Loss: 5273.6772\n",
            "Epoch [140/1000], Loss: 3926.6453\n",
            "Epoch [140/1000], Loss: 3984.4482\n",
            "Epoch [140/1000], Loss: 4298.4443\n",
            "Epoch [140/1000], Loss: 4917.7041\n",
            "Epoch [140/1000], Loss: 3610.9849\n",
            "Epoch [140/1000], Loss: 6680.3999\n",
            "Epoch [140/1000], Loss: 1766.8304\n",
            "Epoch [150/1000], Loss: 4218.2012\n",
            "Epoch [150/1000], Loss: 5096.0605\n",
            "Epoch [150/1000], Loss: 4081.1814\n",
            "Epoch [150/1000], Loss: 3903.718\n",
            "Epoch [150/1000], Loss: 3941.5632\n",
            "Epoch [150/1000], Loss: 4350.8706\n",
            "Epoch [150/1000], Loss: 4980.562\n",
            "Epoch [150/1000], Loss: 4174.5107\n",
            "Epoch [160/1000], Loss: 3335.7178\n",
            "Epoch [160/1000], Loss: 4163.3423\n",
            "Epoch [160/1000], Loss: 4469.293\n",
            "Epoch [160/1000], Loss: 4456.7695\n",
            "Epoch [160/1000], Loss: 4850.9463\n",
            "Epoch [160/1000], Loss: 2993.073\n",
            "Epoch [160/1000], Loss: 5046.5786\n",
            "Epoch [160/1000], Loss: 3903.4917\n",
            "Epoch [170/1000], Loss: 3879.7761\n",
            "Epoch [170/1000], Loss: 3852.4043\n",
            "Epoch [170/1000], Loss: 4951.29\n",
            "Epoch [170/1000], Loss: 3391.7549\n",
            "Epoch [170/1000], Loss: 2611.1006\n",
            "Epoch [170/1000], Loss: 4540.8516\n",
            "Epoch [170/1000], Loss: 5259.916\n",
            "Epoch [170/1000], Loss: 6237.0605\n",
            "Epoch [180/1000], Loss: 3558.6892\n",
            "Epoch [180/1000], Loss: 4776.9673\n",
            "Epoch [180/1000], Loss: 3903.3918\n",
            "Epoch [180/1000], Loss: 3737.0754\n",
            "Epoch [180/1000], Loss: 4449.1772\n",
            "Epoch [180/1000], Loss: 3647.8762\n",
            "Epoch [180/1000], Loss: 3930.3992\n",
            "Epoch [180/1000], Loss: 7767.9668\n",
            "Epoch [190/1000], Loss: 4101.3667\n",
            "Epoch [190/1000], Loss: 4041.9956\n",
            "Epoch [190/1000], Loss: 2981.4041\n",
            "Epoch [190/1000], Loss: 4837.4082\n",
            "Epoch [190/1000], Loss: 3625.74\n",
            "Epoch [190/1000], Loss: 4038.8762\n",
            "Epoch [190/1000], Loss: 4108.1245\n",
            "Epoch [190/1000], Loss: 5829.332\n",
            "Epoch [200/1000], Loss: 4028.9773\n",
            "Epoch [200/1000], Loss: 4505.5015\n",
            "Epoch [200/1000], Loss: 3634.7349\n",
            "Epoch [200/1000], Loss: 3679.7456\n",
            "Epoch [200/1000], Loss: 4365.4858\n",
            "Epoch [200/1000], Loss: 4034.2268\n",
            "Epoch [200/1000], Loss: 3245.062\n",
            "Epoch [200/1000], Loss: 4263.6357\n",
            "Epoch [210/1000], Loss: 3524.198\n",
            "Epoch [210/1000], Loss: 4020.1082\n",
            "Epoch [210/1000], Loss: 3499.4377\n",
            "Epoch [210/1000], Loss: 3651.8931\n",
            "Epoch [210/1000], Loss: 4080.6072\n",
            "Epoch [210/1000], Loss: 4849.5264\n",
            "Epoch [210/1000], Loss: 3587.2661\n",
            "Epoch [210/1000], Loss: 3680.6182\n",
            "Epoch [220/1000], Loss: 2906.7056\n",
            "Epoch [220/1000], Loss: 4229.1953\n",
            "Epoch [220/1000], Loss: 3428.9705\n",
            "Epoch [220/1000], Loss: 4446.0469\n",
            "Epoch [220/1000], Loss: 2897.9333\n",
            "Epoch [220/1000], Loss: 4567.6836\n",
            "Epoch [220/1000], Loss: 4459.6919\n",
            "Epoch [220/1000], Loss: 2577.4055\n",
            "Epoch [230/1000], Loss: 4141.7188\n",
            "Epoch [230/1000], Loss: 3049.6812\n",
            "Epoch [230/1000], Loss: 4217.1558\n",
            "Epoch [230/1000], Loss: 3114.5393\n",
            "Epoch [230/1000], Loss: 3235.9548\n",
            "Epoch [230/1000], Loss: 4710.8735\n",
            "Epoch [230/1000], Loss: 4004.0999\n",
            "Epoch [230/1000], Loss: 5176.4932\n",
            "Epoch [240/1000], Loss: 3779.9424\n",
            "Epoch [240/1000], Loss: 4539.375\n",
            "Epoch [240/1000], Loss: 3737.0081\n",
            "Epoch [240/1000], Loss: 3421.1831\n",
            "Epoch [240/1000], Loss: 3643.3997\n",
            "Epoch [240/1000], Loss: 2860.2305\n",
            "Epoch [240/1000], Loss: 4394.606\n",
            "Epoch [240/1000], Loss: 1886.1003\n",
            "Epoch [250/1000], Loss: 3483.3875\n",
            "Epoch [250/1000], Loss: 3555.0034\n",
            "Epoch [250/1000], Loss: 3975.9761\n",
            "Epoch [250/1000], Loss: 3481.2192\n",
            "Epoch [250/1000], Loss: 3654.8879\n",
            "Epoch [250/1000], Loss: 3958.9736\n",
            "Epoch [250/1000], Loss: 3983.8542\n",
            "Epoch [250/1000], Loss: 1725.0165\n",
            "Epoch [260/1000], Loss: 3058.001\n",
            "Epoch [260/1000], Loss: 3941.3081\n",
            "Epoch [260/1000], Loss: 3555.3906\n",
            "Epoch [260/1000], Loss: 3274.0034\n",
            "Epoch [260/1000], Loss: 3971.988\n",
            "Epoch [260/1000], Loss: 4371.2344\n",
            "Epoch [260/1000], Loss: 3212.9255\n",
            "Epoch [260/1000], Loss: 9604.4414\n",
            "Epoch [270/1000], Loss: 4485.9541\n",
            "Epoch [270/1000], Loss: 3838.9658\n",
            "Epoch [270/1000], Loss: 3984.1714\n",
            "Epoch [270/1000], Loss: 2699.3396\n",
            "Epoch [270/1000], Loss: 3560.9336\n",
            "Epoch [270/1000], Loss: 2812.4783\n",
            "Epoch [270/1000], Loss: 4103.3413\n",
            "Epoch [270/1000], Loss: 4000.6462\n",
            "Epoch [280/1000], Loss: 3211.7124\n",
            "Epoch [280/1000], Loss: 3224.8247\n",
            "Epoch [280/1000], Loss: 3747.7424\n",
            "Epoch [280/1000], Loss: 4554.4888\n",
            "Epoch [280/1000], Loss: 3636.9917\n",
            "Epoch [280/1000], Loss: 3730.3855\n",
            "Epoch [280/1000], Loss: 3333.2014\n",
            "Epoch [280/1000], Loss: 970.953\n",
            "Epoch [290/1000], Loss: 3697.6328\n",
            "Epoch [290/1000], Loss: 4562.752\n",
            "Epoch [290/1000], Loss: 4043.0552\n",
            "Epoch [290/1000], Loss: 3051.2302\n",
            "Epoch [290/1000], Loss: 3029.1479\n",
            "Epoch [290/1000], Loss: 3413.8386\n",
            "Epoch [290/1000], Loss: 3165.5481\n",
            "Epoch [290/1000], Loss: 5361.4087\n",
            "Epoch [300/1000], Loss: 3992.1084\n",
            "Epoch [300/1000], Loss: 3206.5869\n",
            "Epoch [300/1000], Loss: 3440.1331\n",
            "Epoch [300/1000], Loss: 3252.8464\n",
            "Epoch [300/1000], Loss: 3797.4683\n",
            "Epoch [300/1000], Loss: 3818.9353\n",
            "Epoch [300/1000], Loss: 3388.0493\n",
            "Epoch [300/1000], Loss: 3049.9766\n",
            "Epoch [310/1000], Loss: 3383.7695\n",
            "Epoch [310/1000], Loss: 3128.4805\n",
            "Epoch [310/1000], Loss: 3781.7231\n",
            "Epoch [310/1000], Loss: 3752.5549\n",
            "Epoch [310/1000], Loss: 2927.9297\n",
            "Epoch [310/1000], Loss: 3180.625\n",
            "Epoch [310/1000], Loss: 4633.3774\n",
            "Epoch [310/1000], Loss: 1347.1692\n",
            "Epoch [320/1000], Loss: 3789.9487\n",
            "Epoch [320/1000], Loss: 2988.1892\n",
            "Epoch [320/1000], Loss: 2761.7344\n",
            "Epoch [320/1000], Loss: 3200.0806\n",
            "Epoch [320/1000], Loss: 4200.6045\n",
            "Epoch [320/1000], Loss: 3765.3806\n",
            "Epoch [320/1000], Loss: 3947.3149\n",
            "Epoch [320/1000], Loss: 152.7764\n",
            "Epoch [330/1000], Loss: 3275.5798\n",
            "Epoch [330/1000], Loss: 3355.2471\n",
            "Epoch [330/1000], Loss: 3358.6597\n",
            "Epoch [330/1000], Loss: 3290.0747\n",
            "Epoch [330/1000], Loss: 3709.8752\n",
            "Epoch [330/1000], Loss: 3781.9143\n",
            "Epoch [330/1000], Loss: 3427.2729\n",
            "Epoch [330/1000], Loss: 4413.3672\n",
            "Epoch [340/1000], Loss: 3315.259\n",
            "Epoch [340/1000], Loss: 3166.7493\n",
            "Epoch [340/1000], Loss: 4399.6787\n",
            "Epoch [340/1000], Loss: 3244.854\n",
            "Epoch [340/1000], Loss: 2971.5066\n",
            "Epoch [340/1000], Loss: 2682.8862\n",
            "Epoch [340/1000], Loss: 4446.7861\n",
            "Epoch [340/1000], Loss: 895.6371\n",
            "Epoch [350/1000], Loss: 3449.0742\n",
            "Epoch [350/1000], Loss: 3636.8298\n",
            "Epoch [350/1000], Loss: 3482.04\n",
            "Epoch [350/1000], Loss: 3730.3386\n",
            "Epoch [350/1000], Loss: 3710.7544\n",
            "Epoch [350/1000], Loss: 2661.7556\n",
            "Epoch [350/1000], Loss: 3379.8369\n",
            "Epoch [350/1000], Loss: 1175.3105\n",
            "Epoch [360/1000], Loss: 3698.0193\n",
            "Epoch [360/1000], Loss: 2571.7434\n",
            "Epoch [360/1000], Loss: 2869.5168\n",
            "Epoch [360/1000], Loss: 3557.2292\n",
            "Epoch [360/1000], Loss: 3606.9578\n",
            "Epoch [360/1000], Loss: 3624.009\n",
            "Epoch [360/1000], Loss: 3996.5256\n",
            "Epoch [360/1000], Loss: 799.3915\n",
            "Epoch [370/1000], Loss: 3962.7686\n",
            "Epoch [370/1000], Loss: 3939.3462\n",
            "Epoch [370/1000], Loss: 3484.8494\n",
            "Epoch [370/1000], Loss: 3288.0894\n",
            "Epoch [370/1000], Loss: 2656.8831\n",
            "Epoch [370/1000], Loss: 2771.6792\n",
            "Epoch [370/1000], Loss: 3196.4768\n",
            "Epoch [370/1000], Loss: 8350.4512\n",
            "Epoch [380/1000], Loss: 2856.7368\n",
            "Epoch [380/1000], Loss: 3330.1499\n",
            "Epoch [380/1000], Loss: 3540.1479\n",
            "Epoch [380/1000], Loss: 3084.8274\n",
            "Epoch [380/1000], Loss: 3536.738\n",
            "Epoch [380/1000], Loss: 3135.2739\n",
            "Epoch [380/1000], Loss: 3985.3298\n",
            "Epoch [380/1000], Loss: 3294.6511\n",
            "Epoch [390/1000], Loss: 3204.8308\n",
            "Epoch [390/1000], Loss: 2933.1536\n",
            "Epoch [390/1000], Loss: 3270.6533\n",
            "Epoch [390/1000], Loss: 3493.5386\n",
            "Epoch [390/1000], Loss: 2843.3811\n",
            "Epoch [390/1000], Loss: 3797.2842\n",
            "Epoch [390/1000], Loss: 3818.9336\n",
            "Epoch [390/1000], Loss: 2435.1826\n",
            "Epoch [400/1000], Loss: 3506.9946\n",
            "Epoch [400/1000], Loss: 3082.6428\n",
            "Epoch [400/1000], Loss: 3367.085\n",
            "Epoch [400/1000], Loss: 2849.8093\n",
            "Epoch [400/1000], Loss: 4025.7246\n",
            "Epoch [400/1000], Loss: 3922.438\n",
            "Epoch [400/1000], Loss: 2533.842\n",
            "Epoch [400/1000], Loss: 1173.0078\n",
            "Epoch [410/1000], Loss: 2774.2874\n",
            "Epoch [410/1000], Loss: 3053.4661\n",
            "Epoch [410/1000], Loss: 2797.563\n",
            "Epoch [410/1000], Loss: 3490.1375\n",
            "Epoch [410/1000], Loss: 2378.5144\n",
            "Epoch [410/1000], Loss: 4163.8022\n",
            "Epoch [410/1000], Loss: 4533.041\n",
            "Epoch [410/1000], Loss: 644.7516\n",
            "Epoch [420/1000], Loss: 3350.1086\n",
            "Epoch [420/1000], Loss: 3261.04\n",
            "Epoch [420/1000], Loss: 2132.7078\n",
            "Epoch [420/1000], Loss: 2850.4512\n",
            "Epoch [420/1000], Loss: 3711.2791\n",
            "Epoch [420/1000], Loss: 3700.4675\n",
            "Epoch [420/1000], Loss: 4066.2976\n",
            "Epoch [420/1000], Loss: 487.0653\n",
            "Epoch [430/1000], Loss: 3600.3318\n",
            "Epoch [430/1000], Loss: 2424.5776\n",
            "Epoch [430/1000], Loss: 2920.8352\n",
            "Epoch [430/1000], Loss: 3771.6724\n",
            "Epoch [430/1000], Loss: 4135.8042\n",
            "Epoch [430/1000], Loss: 3243.6238\n",
            "Epoch [430/1000], Loss: 2836.1294\n",
            "Epoch [430/1000], Loss: 597.1732\n",
            "Epoch [440/1000], Loss: 2906.228\n",
            "Epoch [440/1000], Loss: 2527.0334\n",
            "Epoch [440/1000], Loss: 3261.2354\n",
            "Epoch [440/1000], Loss: 3456.2346\n",
            "Epoch [440/1000], Loss: 3432.0854\n",
            "Epoch [440/1000], Loss: 3276.5813\n",
            "Epoch [440/1000], Loss: 3742.708\n",
            "Epoch [440/1000], Loss: 4498.7021\n",
            "Epoch [450/1000], Loss: 3812.7405\n",
            "Epoch [450/1000], Loss: 3668.4602\n",
            "Epoch [450/1000], Loss: 3842.1628\n",
            "Epoch [450/1000], Loss: 2870.8547\n",
            "Epoch [450/1000], Loss: 3117.2805\n",
            "Epoch [450/1000], Loss: 2689.4287\n",
            "Epoch [450/1000], Loss: 2680.8733\n",
            "Epoch [450/1000], Loss: 965.478\n",
            "Epoch [460/1000], Loss: 3419.0469\n",
            "Epoch [460/1000], Loss: 3184.6711\n",
            "Epoch [460/1000], Loss: 3066.8337\n",
            "Epoch [460/1000], Loss: 3658.9514\n",
            "Epoch [460/1000], Loss: 3206.4753\n",
            "Epoch [460/1000], Loss: 3456.4827\n",
            "Epoch [460/1000], Loss: 2462.0781\n",
            "Epoch [460/1000], Loss: 2746.0474\n",
            "Epoch [470/1000], Loss: 2613.1687\n",
            "Epoch [470/1000], Loss: 3532.5073\n",
            "Epoch [470/1000], Loss: 2367.9607\n",
            "Epoch [470/1000], Loss: 3511.7913\n",
            "Epoch [470/1000], Loss: 3241.3313\n",
            "Epoch [470/1000], Loss: 3648.6218\n",
            "Epoch [470/1000], Loss: 3477.6443\n",
            "Epoch [470/1000], Loss: 2145.2686\n",
            "Epoch [480/1000], Loss: 3342.8381\n",
            "Epoch [480/1000], Loss: 3217.4175\n",
            "Epoch [480/1000], Loss: 2800.8643\n",
            "Epoch [480/1000], Loss: 2710.1562\n",
            "Epoch [480/1000], Loss: 3761.1155\n",
            "Epoch [480/1000], Loss: 3443.3955\n",
            "Epoch [480/1000], Loss: 2702.3606\n",
            "Epoch [480/1000], Loss: 6919.5151\n",
            "Epoch [490/1000], Loss: 3219.3086\n",
            "Epoch [490/1000], Loss: 2746.7883\n",
            "Epoch [490/1000], Loss: 2934.0615\n",
            "Epoch [490/1000], Loss: 3826.4187\n",
            "Epoch [490/1000], Loss: 3665.8403\n",
            "Epoch [490/1000], Loss: 2775.5847\n",
            "Epoch [490/1000], Loss: 3023.5044\n",
            "Epoch [490/1000], Loss: 1734.4481\n",
            "Epoch [500/1000], Loss: 3223.2512\n",
            "Epoch [500/1000], Loss: 2106.0557\n",
            "Epoch [500/1000], Loss: 2973.2388\n",
            "Epoch [500/1000], Loss: 3263.835\n",
            "Epoch [500/1000], Loss: 3499.6765\n",
            "Epoch [500/1000], Loss: 2959.3516\n",
            "Epoch [500/1000], Loss: 3839.2937\n",
            "Epoch [500/1000], Loss: 5540.9844\n",
            "Epoch [510/1000], Loss: 2931.8699\n",
            "Epoch [510/1000], Loss: 3087.5127\n",
            "Epoch [510/1000], Loss: 2776.6021\n",
            "Epoch [510/1000], Loss: 3424.5908\n",
            "Epoch [510/1000], Loss: 3750.7568\n",
            "Epoch [510/1000], Loss: 2441.0403\n",
            "Epoch [510/1000], Loss: 3682.1687\n",
            "Epoch [510/1000], Loss: 206.0453\n",
            "Epoch [520/1000], Loss: 2305.1443\n",
            "Epoch [520/1000], Loss: 3058.5034\n",
            "Epoch [520/1000], Loss: 3610.1719\n",
            "Epoch [520/1000], Loss: 3382.2717\n",
            "Epoch [520/1000], Loss: 3686.4436\n",
            "Epoch [520/1000], Loss: 3363.105\n",
            "Epoch [520/1000], Loss: 2512.3909\n",
            "Epoch [520/1000], Loss: 1790.1201\n",
            "Epoch [530/1000], Loss: 3298.0664\n",
            "Epoch [530/1000], Loss: 3318.5256\n",
            "Epoch [530/1000], Loss: 3265.1028\n",
            "Epoch [530/1000], Loss: 3556.0894\n",
            "Epoch [530/1000], Loss: 1955.651\n",
            "Epoch [530/1000], Loss: 2462.4023\n",
            "Epoch [530/1000], Loss: 3696.6521\n",
            "Epoch [530/1000], Loss: 6534.2061\n",
            "Epoch [540/1000], Loss: 3078.9856\n",
            "Epoch [540/1000], Loss: 3580.8618\n",
            "Epoch [540/1000], Loss: 3225.5325\n",
            "Epoch [540/1000], Loss: 3021.5686\n",
            "Epoch [540/1000], Loss: 2846.7339\n",
            "Epoch [540/1000], Loss: 2850.2456\n",
            "Epoch [540/1000], Loss: 3222.9968\n",
            "Epoch [540/1000], Loss: 826.8674\n",
            "Epoch [550/1000], Loss: 3054.9062\n",
            "Epoch [550/1000], Loss: 3007.0269\n",
            "Epoch [550/1000], Loss: 3473.54\n",
            "Epoch [550/1000], Loss: 2930.1062\n",
            "Epoch [550/1000], Loss: 3545.4546\n",
            "Epoch [550/1000], Loss: 2945.2124\n",
            "Epoch [550/1000], Loss: 2554.7271\n",
            "Epoch [550/1000], Loss: 4494.9951\n",
            "Epoch [560/1000], Loss: 2892.3098\n",
            "Epoch [560/1000], Loss: 2917.0876\n",
            "Epoch [560/1000], Loss: 2308.9604\n",
            "Epoch [560/1000], Loss: 4169.4336\n",
            "Epoch [560/1000], Loss: 3284.3931\n",
            "Epoch [560/1000], Loss: 3519.7786\n",
            "Epoch [560/1000], Loss: 2545.3828\n",
            "Epoch [560/1000], Loss: 1358.1632\n",
            "Epoch [570/1000], Loss: 3112.5959\n",
            "Epoch [570/1000], Loss: 2059.3428\n",
            "Epoch [570/1000], Loss: 1975.1704\n",
            "Epoch [570/1000], Loss: 3015.1924\n",
            "Epoch [570/1000], Loss: 3218.6721\n",
            "Epoch [570/1000], Loss: 3911.5759\n",
            "Epoch [570/1000], Loss: 4191.6938\n",
            "Epoch [570/1000], Loss: 2782.0105\n",
            "Epoch [580/1000], Loss: 3189.8906\n",
            "Epoch [580/1000], Loss: 2653.5933\n",
            "Epoch [580/1000], Loss: 2732.6233\n",
            "Epoch [580/1000], Loss: 3206.1287\n",
            "Epoch [580/1000], Loss: 3669.9912\n",
            "Epoch [580/1000], Loss: 3210.7446\n",
            "Epoch [580/1000], Loss: 2823.4719\n",
            "Epoch [580/1000], Loss: 1915.5717\n",
            "Epoch [590/1000], Loss: 3272.6699\n",
            "Epoch [590/1000], Loss: 2016.9191\n",
            "Epoch [590/1000], Loss: 3303.9978\n",
            "Epoch [590/1000], Loss: 2787.5002\n",
            "Epoch [590/1000], Loss: 3755.4961\n",
            "Epoch [590/1000], Loss: 2767.9487\n",
            "Epoch [590/1000], Loss: 3354.8354\n",
            "Epoch [590/1000], Loss: 4604.2969\n",
            "Epoch [600/1000], Loss: 2584.5061\n",
            "Epoch [600/1000], Loss: 2794.1423\n",
            "Epoch [600/1000], Loss: 1981.553\n",
            "Epoch [600/1000], Loss: 3024.7749\n",
            "Epoch [600/1000], Loss: 3149.2493\n",
            "Epoch [600/1000], Loss: 3653.9656\n",
            "Epoch [600/1000], Loss: 4046.9536\n",
            "Epoch [600/1000], Loss: 4253.9937\n",
            "Epoch [610/1000], Loss: 3638.7708\n",
            "Epoch [610/1000], Loss: 2882.1868\n",
            "Epoch [610/1000], Loss: 2591.25\n",
            "Epoch [610/1000], Loss: 2356.6892\n",
            "Epoch [610/1000], Loss: 3146.1855\n",
            "Epoch [610/1000], Loss: 2752.3264\n",
            "Epoch [610/1000], Loss: 4001.5737\n",
            "Epoch [610/1000], Loss: 1178.2495\n",
            "Epoch [620/1000], Loss: 2535.6643\n",
            "Epoch [620/1000], Loss: 3668.9229\n",
            "Epoch [620/1000], Loss: 2101.821\n",
            "Epoch [620/1000], Loss: 3922.9363\n",
            "Epoch [620/1000], Loss: 2997.9399\n",
            "Epoch [620/1000], Loss: 2640.498\n",
            "Epoch [620/1000], Loss: 3408.5337\n",
            "Epoch [620/1000], Loss: 1913.6459\n",
            "Epoch [630/1000], Loss: 3449.1519\n",
            "Epoch [630/1000], Loss: 3558.0938\n",
            "Epoch [630/1000], Loss: 2208.8281\n",
            "Epoch [630/1000], Loss: 3942.6768\n",
            "Epoch [630/1000], Loss: 2408.9983\n",
            "Epoch [630/1000], Loss: 2679.8206\n",
            "Epoch [630/1000], Loss: 2800.6096\n",
            "Epoch [630/1000], Loss: 4772.4482\n",
            "Epoch [640/1000], Loss: 2479.5525\n",
            "Epoch [640/1000], Loss: 2922.22\n",
            "Epoch [640/1000], Loss: 4005.468\n",
            "Epoch [640/1000], Loss: 2747.4272\n",
            "Epoch [640/1000], Loss: 2709.4299\n",
            "Epoch [640/1000], Loss: 3754.8472\n",
            "Epoch [640/1000], Loss: 2358.0874\n",
            "Epoch [640/1000], Loss: 5051.6143\n",
            "Epoch [650/1000], Loss: 2427.3867\n",
            "Epoch [650/1000], Loss: 2316.3762\n",
            "Epoch [650/1000], Loss: 3181.665\n",
            "Epoch [650/1000], Loss: 3352.0166\n",
            "Epoch [650/1000], Loss: 3537.4602\n",
            "Epoch [650/1000], Loss: 3493.4736\n",
            "Epoch [650/1000], Loss: 2754.1909\n",
            "Epoch [650/1000], Loss: 2999.939\n",
            "Epoch [660/1000], Loss: 2289.8977\n",
            "Epoch [660/1000], Loss: 2368.6428\n",
            "Epoch [660/1000], Loss: 3911.26\n",
            "Epoch [660/1000], Loss: 2610.0205\n",
            "Epoch [660/1000], Loss: 3930.2527\n",
            "Epoch [660/1000], Loss: 2676.9219\n",
            "Epoch [660/1000], Loss: 3303.7795\n",
            "Epoch [660/1000], Loss: 1777.2178\n",
            "Epoch [670/1000], Loss: 3689.7727\n",
            "Epoch [670/1000], Loss: 3899.4441\n",
            "Epoch [670/1000], Loss: 2953.9619\n",
            "Epoch [670/1000], Loss: 1845.2725\n",
            "Epoch [670/1000], Loss: 3205.0464\n",
            "Epoch [670/1000], Loss: 2434.7363\n",
            "Epoch [670/1000], Loss: 2877.9988\n",
            "Epoch [670/1000], Loss: 4169.8281\n",
            "Epoch [680/1000], Loss: 2300.7961\n",
            "Epoch [680/1000], Loss: 2386.3467\n",
            "Epoch [680/1000], Loss: 3254.1301\n",
            "Epoch [680/1000], Loss: 3463.0229\n",
            "Epoch [680/1000], Loss: 2968.6931\n",
            "Epoch [680/1000], Loss: 2475.0422\n",
            "Epoch [680/1000], Loss: 4206.0034\n",
            "Epoch [680/1000], Loss: 824.9938\n",
            "Epoch [690/1000], Loss: 2650.6963\n",
            "Epoch [690/1000], Loss: 3712.697\n",
            "Epoch [690/1000], Loss: 2965.7219\n",
            "Epoch [690/1000], Loss: 3160.6675\n",
            "Epoch [690/1000], Loss: 3166.4011\n",
            "Epoch [690/1000], Loss: 3015.3428\n",
            "Epoch [690/1000], Loss: 2356.645\n",
            "Epoch [690/1000], Loss: 659.3809\n",
            "Epoch [700/1000], Loss: 2895.5962\n",
            "Epoch [700/1000], Loss: 3436.8511\n",
            "Epoch [700/1000], Loss: 2811.0679\n",
            "Epoch [700/1000], Loss: 3644.7556\n",
            "Epoch [700/1000], Loss: 3032.1587\n",
            "Epoch [700/1000], Loss: 2382.4465\n",
            "Epoch [700/1000], Loss: 2540.6572\n",
            "Epoch [700/1000], Loss: 4902.6548\n",
            "Epoch [710/1000], Loss: 3169.8008\n",
            "Epoch [710/1000], Loss: 2283.2764\n",
            "Epoch [710/1000], Loss: 2250.2148\n",
            "Epoch [710/1000], Loss: 3470.2593\n",
            "Epoch [710/1000], Loss: 2448.7561\n",
            "Epoch [710/1000], Loss: 3233.0542\n",
            "Epoch [710/1000], Loss: 4075.0171\n",
            "Epoch [710/1000], Loss: 1315.2908\n",
            "Epoch [720/1000], Loss: 3258.6343\n",
            "Epoch [720/1000], Loss: 2462.9722\n",
            "Epoch [720/1000], Loss: 2430.0024\n",
            "Epoch [720/1000], Loss: 3188.2231\n",
            "Epoch [720/1000], Loss: 3041.6165\n",
            "Epoch [720/1000], Loss: 3008.9814\n",
            "Epoch [720/1000], Loss: 3170.9143\n",
            "Epoch [720/1000], Loss: 6865.6201\n",
            "Epoch [730/1000], Loss: 2277.6829\n",
            "Epoch [730/1000], Loss: 3463.188\n",
            "Epoch [730/1000], Loss: 2887.3994\n",
            "Epoch [730/1000], Loss: 2953.2644\n",
            "Epoch [730/1000], Loss: 2626.2556\n",
            "Epoch [730/1000], Loss: 3591.0015\n",
            "Epoch [730/1000], Loss: 2866.4849\n",
            "Epoch [730/1000], Loss: 4702.7534\n",
            "Epoch [740/1000], Loss: 3075.7449\n",
            "Epoch [740/1000], Loss: 3457.8538\n",
            "Epoch [740/1000], Loss: 2968.2625\n",
            "Epoch [740/1000], Loss: 3479.2991\n",
            "Epoch [740/1000], Loss: 2482.9971\n",
            "Epoch [740/1000], Loss: 2352.9746\n",
            "Epoch [740/1000], Loss: 2953.1475\n",
            "Epoch [740/1000], Loss: 2690.0405\n",
            "Epoch [750/1000], Loss: 3683.7456\n",
            "Epoch [750/1000], Loss: 2681.0959\n",
            "Epoch [750/1000], Loss: 2471.6257\n",
            "Epoch [750/1000], Loss: 2875.2869\n",
            "Epoch [750/1000], Loss: 2413.9006\n",
            "Epoch [750/1000], Loss: 3131.9624\n",
            "Epoch [750/1000], Loss: 3395.0312\n",
            "Epoch [750/1000], Loss: 4070.6328\n",
            "Epoch [760/1000], Loss: 3705.0886\n",
            "Epoch [760/1000], Loss: 3062.374\n",
            "Epoch [760/1000], Loss: 2324.2009\n",
            "Epoch [760/1000], Loss: 2827.8718\n",
            "Epoch [760/1000], Loss: 3141.5273\n",
            "Epoch [760/1000], Loss: 3079.9866\n",
            "Epoch [760/1000], Loss: 2615.5056\n",
            "Epoch [760/1000], Loss: 2037.012\n",
            "Epoch [770/1000], Loss: 2844.572\n",
            "Epoch [770/1000], Loss: 2964.4285\n",
            "Epoch [770/1000], Loss: 2551.4509\n",
            "Epoch [770/1000], Loss: 3575.572\n",
            "Epoch [770/1000], Loss: 3113.3699\n",
            "Epoch [770/1000], Loss: 2976.9177\n",
            "Epoch [770/1000], Loss: 2421.6191\n",
            "Epoch [770/1000], Loss: 6671.4253\n",
            "Epoch [780/1000], Loss: 3373.938\n",
            "Epoch [780/1000], Loss: 2544.9702\n",
            "Epoch [780/1000], Loss: 2477.9758\n",
            "Epoch [780/1000], Loss: 3034.0393\n",
            "Epoch [780/1000], Loss: 3158.0894\n",
            "Epoch [780/1000], Loss: 3122.8208\n",
            "Epoch [780/1000], Loss: 3042.2268\n",
            "Epoch [780/1000], Loss: 1597.3618\n",
            "Epoch [790/1000], Loss: 2818.1038\n",
            "Epoch [790/1000], Loss: 2127.8496\n",
            "Epoch [790/1000], Loss: 3422.0081\n",
            "Epoch [790/1000], Loss: 3205.0874\n",
            "Epoch [790/1000], Loss: 3012.6824\n",
            "Epoch [790/1000], Loss: 3360.0249\n",
            "Epoch [790/1000], Loss: 2542.1011\n",
            "Epoch [790/1000], Loss: 5307.876\n",
            "Epoch [800/1000], Loss: 3464.1653\n",
            "Epoch [800/1000], Loss: 2534.1111\n",
            "Epoch [800/1000], Loss: 2857.0762\n",
            "Epoch [800/1000], Loss: 2830.5037\n",
            "Epoch [800/1000], Loss: 2808.1841\n",
            "Epoch [800/1000], Loss: 2590.3601\n",
            "Epoch [800/1000], Loss: 3557.6167\n",
            "Epoch [800/1000], Loss: 2360.6775\n",
            "Epoch [810/1000], Loss: 2544.9097\n",
            "Epoch [810/1000], Loss: 3757.708\n",
            "Epoch [810/1000], Loss: 3699.4792\n",
            "Epoch [810/1000], Loss: 2777.8923\n",
            "Epoch [810/1000], Loss: 2370.5283\n",
            "Epoch [810/1000], Loss: 3218.5505\n",
            "Epoch [810/1000], Loss: 2222.3452\n",
            "Epoch [810/1000], Loss: 3298.9673\n",
            "Epoch [820/1000], Loss: 3357.0938\n",
            "Epoch [820/1000], Loss: 2489.7471\n",
            "Epoch [820/1000], Loss: 2843.1394\n",
            "Epoch [820/1000], Loss: 2527.2874\n",
            "Epoch [820/1000], Loss: 2859.6479\n",
            "Epoch [820/1000], Loss: 3209.1143\n",
            "Epoch [820/1000], Loss: 3454.4639\n",
            "Epoch [820/1000], Loss: 314.2238\n",
            "Epoch [830/1000], Loss: 3107.6499\n",
            "Epoch [830/1000], Loss: 3503.3604\n",
            "Epoch [830/1000], Loss: 3227.5161\n",
            "Epoch [830/1000], Loss: 2931.2422\n",
            "Epoch [830/1000], Loss: 2138.2478\n",
            "Epoch [830/1000], Loss: 2791.4636\n",
            "Epoch [830/1000], Loss: 2861.4893\n",
            "Epoch [830/1000], Loss: 3016.7598\n",
            "Epoch [840/1000], Loss: 3482.6646\n",
            "Epoch [840/1000], Loss: 2465.5073\n",
            "Epoch [840/1000], Loss: 3918.4302\n",
            "Epoch [840/1000], Loss: 2999.323\n",
            "Epoch [840/1000], Loss: 2443.918\n",
            "Epoch [840/1000], Loss: 2533.4934\n",
            "Epoch [840/1000], Loss: 2791.6631\n",
            "Epoch [840/1000], Loss: 1433.9202\n",
            "Epoch [850/1000], Loss: 2577.5994\n",
            "Epoch [850/1000], Loss: 4752.5386\n",
            "Epoch [850/1000], Loss: 2701.6765\n",
            "Epoch [850/1000], Loss: 1836.0756\n",
            "Epoch [850/1000], Loss: 3330.2998\n",
            "Epoch [850/1000], Loss: 2435.5452\n",
            "Epoch [850/1000], Loss: 2786.04\n",
            "Epoch [850/1000], Loss: 5110.3862\n",
            "Epoch [860/1000], Loss: 2731.8977\n",
            "Epoch [860/1000], Loss: 3691.783\n",
            "Epoch [860/1000], Loss: 2789.3\n",
            "Epoch [860/1000], Loss: 2660.9185\n",
            "Epoch [860/1000], Loss: 2300.0164\n",
            "Epoch [860/1000], Loss: 3966.9617\n",
            "Epoch [860/1000], Loss: 2502.5994\n",
            "Epoch [860/1000], Loss: 1030.667\n",
            "Epoch [870/1000], Loss: 3587.6038\n",
            "Epoch [870/1000], Loss: 2658.0962\n",
            "Epoch [870/1000], Loss: 2850.0002\n",
            "Epoch [870/1000], Loss: 2661.7827\n",
            "Epoch [870/1000], Loss: 2729.6494\n",
            "Epoch [870/1000], Loss: 3150.7812\n",
            "Epoch [870/1000], Loss: 2920.8418\n",
            "Epoch [870/1000], Loss: 2082.1868\n",
            "Epoch [880/1000], Loss: 2869.6658\n",
            "Epoch [880/1000], Loss: 2961.8611\n",
            "Epoch [880/1000], Loss: 3095.4292\n",
            "Epoch [880/1000], Loss: 2563.6064\n",
            "Epoch [880/1000], Loss: 1937.5896\n",
            "Epoch [880/1000], Loss: 3125.5081\n",
            "Epoch [880/1000], Loss: 3263.9124\n",
            "Epoch [880/1000], Loss: 14061.8779\n",
            "Epoch [890/1000], Loss: 3186.5244\n",
            "Epoch [890/1000], Loss: 2554.5027\n",
            "Epoch [890/1000], Loss: 2491.593\n",
            "Epoch [890/1000], Loss: 2708.759\n",
            "Epoch [890/1000], Loss: 3342.5972\n",
            "Epoch [890/1000], Loss: 3341.8315\n",
            "Epoch [890/1000], Loss: 2612.1814\n",
            "Epoch [890/1000], Loss: 7146.5269\n",
            "Epoch [900/1000], Loss: 3391.3931\n",
            "Epoch [900/1000], Loss: 2494.6206\n",
            "Epoch [900/1000], Loss: 3329.645\n",
            "Epoch [900/1000], Loss: 3895.8645\n",
            "Epoch [900/1000], Loss: 2157.4312\n",
            "Epoch [900/1000], Loss: 2250.4041\n",
            "Epoch [900/1000], Loss: 3018.4705\n",
            "Epoch [900/1000], Loss: 1906.1912\n",
            "Epoch [910/1000], Loss: 2605.3516\n",
            "Epoch [910/1000], Loss: 3109.1187\n",
            "Epoch [910/1000], Loss: 3406.6494\n",
            "Epoch [910/1000], Loss: 1899.2437\n",
            "Epoch [910/1000], Loss: 2668.8662\n",
            "Epoch [910/1000], Loss: 3340.0728\n",
            "Epoch [910/1000], Loss: 3531.2527\n",
            "Epoch [910/1000], Loss: 1569.7338\n",
            "Epoch [920/1000], Loss: 3494.8831\n",
            "Epoch [920/1000], Loss: 2745.5469\n",
            "Epoch [920/1000], Loss: 2832.1472\n",
            "Epoch [920/1000], Loss: 2622.8518\n",
            "Epoch [920/1000], Loss: 2226.4268\n",
            "Epoch [920/1000], Loss: 4179.3701\n",
            "Epoch [920/1000], Loss: 2427.8018\n",
            "Epoch [920/1000], Loss: 1826.0004\n",
            "Epoch [930/1000], Loss: 3116.1829\n",
            "Epoch [930/1000], Loss: 2423.158\n",
            "Epoch [930/1000], Loss: 3321.8452\n",
            "Epoch [930/1000], Loss: 2546.3589\n",
            "Epoch [930/1000], Loss: 3316.9287\n",
            "Epoch [930/1000], Loss: 2777.2344\n",
            "Epoch [930/1000], Loss: 2815.0425\n",
            "Epoch [930/1000], Loss: 4923.709\n",
            "Epoch [940/1000], Loss: 3078.0049\n",
            "Epoch [940/1000], Loss: 2729.1438\n",
            "Epoch [940/1000], Loss: 3089.7456\n",
            "Epoch [940/1000], Loss: 3081.1353\n",
            "Epoch [940/1000], Loss: 2980.8867\n",
            "Epoch [940/1000], Loss: 2951.0833\n",
            "Epoch [940/1000], Loss: 2661.7283\n",
            "Epoch [940/1000], Loss: 752.4125\n",
            "Epoch [950/1000], Loss: 3754.853\n",
            "Epoch [950/1000], Loss: 3042.6858\n",
            "Epoch [950/1000], Loss: 2406.1663\n",
            "Epoch [950/1000], Loss: 3475.0117\n",
            "Epoch [950/1000], Loss: 2712.4419\n",
            "Epoch [950/1000], Loss: 2569.7461\n",
            "Epoch [950/1000], Loss: 2474.4944\n",
            "Epoch [950/1000], Loss: 2865.5747\n",
            "Epoch [960/1000], Loss: 2404.0166\n",
            "Epoch [960/1000], Loss: 2544.334\n",
            "Epoch [960/1000], Loss: 2549.9141\n",
            "Epoch [960/1000], Loss: 4089.3787\n",
            "Epoch [960/1000], Loss: 3134.3838\n",
            "Epoch [960/1000], Loss: 3098.6807\n",
            "Epoch [960/1000], Loss: 2658.2559\n",
            "Epoch [960/1000], Loss: 2056.031\n",
            "Epoch [970/1000], Loss: 3806.7249\n",
            "Epoch [970/1000], Loss: 3218.2471\n",
            "Epoch [970/1000], Loss: 2649.5381\n",
            "Epoch [970/1000], Loss: 2640.2361\n",
            "Epoch [970/1000], Loss: 3186.1733\n",
            "Epoch [970/1000], Loss: 2262.5562\n",
            "Epoch [970/1000], Loss: 2751.2168\n",
            "Epoch [970/1000], Loss: 1264.999\n",
            "Epoch [980/1000], Loss: 2029.7706\n",
            "Epoch [980/1000], Loss: 2869.5591\n",
            "Epoch [980/1000], Loss: 3272.6565\n",
            "Epoch [980/1000], Loss: 3738.6736\n",
            "Epoch [980/1000], Loss: 2413.437\n",
            "Epoch [980/1000], Loss: 4198.5107\n",
            "Epoch [980/1000], Loss: 1820.3134\n",
            "Epoch [980/1000], Loss: 4414.9082\n",
            "Epoch [990/1000], Loss: 3673.3738\n",
            "Epoch [990/1000], Loss: 2621.8728\n",
            "Epoch [990/1000], Loss: 2649.6086\n",
            "Epoch [990/1000], Loss: 2830.2354\n",
            "Epoch [990/1000], Loss: 2871.9412\n",
            "Epoch [990/1000], Loss: 2879.8198\n",
            "Epoch [990/1000], Loss: 2945.4348\n",
            "Epoch [990/1000], Loss: 1734.0945\n",
            "Epoch [1000/1000], Loss: 2687.0491\n",
            "Epoch [1000/1000], Loss: 3239.5847\n",
            "Epoch [1000/1000], Loss: 2379.0708\n",
            "Epoch [1000/1000], Loss: 3200.582\n",
            "Epoch [1000/1000], Loss: 2761.7197\n",
            "Epoch [1000/1000], Loss: 3346.5596\n",
            "Epoch [1000/1000], Loss: 2863.7444\n",
            "Epoch [1000/1000], Loss: 1525.2617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, by increasing our number of neurons here to 10, the losses have actually increased. However, while the final value for the loss previously was only 699, on average, the losses here are fairly similar if not slightly lower. I'm going to calculate the MSE to investigate this further."
      ],
      "metadata": {
        "id": "sy35Xb9kOAS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUzDHN56OQEV",
        "outputId": "a74fa949-7819-4831-b9fa-731f2d6837f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2853.203125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE has slightly reduced (by 24)"
      ],
      "metadata": {
        "id": "67FdR5qMQzae"
      }
    }
  ]
}